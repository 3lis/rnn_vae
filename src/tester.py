"""
#############################################################################################################

Test routines to evaluate performance of model

    Alice   2019

#############################################################################################################
"""

import  os
import  sys
import  glob
import  deepdish
import  numpy       as np

from    keras       import Model, preprocessing, losses, optimizers
from    keras       import backend as K
from    math        import sqrt, ceil, inf
from    PIL         import Image, ImageFilter

from    exec_dset   import synthia_names
import  mesg        as ms
import  exec_dset


DEBUG0              = True

file_ext            = ( '.png', '.jpg', '.jpeg' )
cond_ext            = lambda x: x.endswith( file_ext )          # the condition an image file must satisfy
seq_dir             = 'imgs'
dir_latent          = 'dataset/synthia/latent'

dset_code_latent    = None                                      # for converting code to latent

# graphic parameters for segmentation visualization
alpha_overlay       = 0.35
#lane_color         = '#FF77FF'
lane_color          = '#FFFF00'
car_color           = '#00FFFF'

# function for checking that one element of Y is contained in X
check_incl          = lambda x, y: bool( [ i for i in y if ( i in x ) ] )
code_city           = ( 'S02', 'S04' )
code_freeway        = ( 'S01', 'S06' )
code_sunny          = ( 'FL_', 'SP_', 'SM_', 'WR_' )
code_dark           = ( 'DW_', 'FG_', 'NT_', 'RN_', 'RT_', 'SR_', 'ST_', 'WN_' )

cnfg                = []                                        # NOTE initialized by 'nn_main.py'


# ===========================================================================================================
#
#   Support functions for image handling
#
#   - imgsize
#   - in_rgb
#   - out_rgb
#
#   - array_to_image
#   - file_to_array
#   - latent_to_image
#   - code_to_latent
#   - code_to_path
#
#   - save_image
#   - make_collage
#   - save_collage
#
#   - img_overlay
#   - img_overlays
#   - get_targ
#
# ===========================================================================================================

def imgsize( model, inp=True ):
    """ -----------------------------------------------------------------------------------------------------
    Return height and width of model input/output

    model:          [keras.engine.training.Model]
    inp:            [bool] if True consider size of input, if False consider size of output

    return:         [list]
    ----------------------------------------------------------------------------------------------------- """
    s   = model.input_shape if inp else model.output_shape

    if isinstance( s, tuple ):
        if len( s ) == 3:
            return s[ :2 ] if inp else s[ :2 ]

        if len( s ) == 4:                           # when a batch dimension is included (as first axis)
            return s[ 1:3 ] if inp else s[ 1:3 ]

    # the model has multiple inputs
    if isinstance( s, list ):
        for i in s:
            if len( i ) == 3:
                return i[ :2 ] if inp else i[ :2 ]

            if len( i ) == 4:                       # when a batch dimension is included (as first axis)
                return i[ 1:3 ] if inp else i[ 1:3 ]



def in_rgb( model ):
    """ -----------------------------------------------------------------------------------------------------
    Return the kind of input image accepted by the model
            True    --> RGB
            False   --> graylevel

    model:          [keras.engine.training.Model]

    return:         [bool]
    ----------------------------------------------------------------------------------------------------- """
    if isinstance( model.input_shape, tuple ):
        return model.input_shape[ -1 ] != 1

    # the model has multiple inputs
    if isinstance( model.input_shape, list ):
        for i in model.input_shape:
            if len( i ) in ( 3, 4 ):
                return i [ -1 ] != 1

        raise ValueError( "Unexpected input shape {}".format( model.input_shape ) )



def out_rgb( model ):
    """ -----------------------------------------------------------------------------------------------------
    Return the kind of output image generated by the model
            True    --> RGB
            False   --> graylevel

    model:          [keras.engine.training.Model]

    return:         [bool]
    ----------------------------------------------------------------------------------------------------- """
    if isinstance( model.output_shape, tuple ):
        return model.output_shape[ -1 ] != 1

    # the model has multiple outputs
    if isinstance( model.output_shape, list ):
        for i in model.output_shape:
            if len( i ) in ( 3, 4 ):
                return i [ -1 ] != 1

        raise ValueError( "Unexpected output shape {}".format( model.output_shape ) )



def array_to_image( array, rgb, rng=255. ):
    """ -----------------------------------------------------------------------------------------------------
    Convert numpy.ndarray to PIL.Image

        NOTE: to actually convert a PIL.Image.Image to numpy.array, just perform a normal cast:
            > np.array( img )
        but this does not work with PIL.Image.PngImageFile

    array:          [numpy.ndarray] pixel values (between 0..1)
    rgb:            [bool] True if RGB, false if grayscale
    rng:            [float] max value of pixel

    return:         [PIL.Image.Image]
    ----------------------------------------------------------------------------------------------------- """
    if len( array.shape ) == 4:
        array   = array[ 0, :, :, : ]                               # remove batch axis

    pixels  = array if ( rgb or len( array.shape ) == 2 ) else array[ :, :, 0 ]

    ptp     = pixels.ptp()
    if ptp:
        pixels  = ( pixels - pixels.min() ) / ptp
    pixels  = rng * pixels                                          # normalization
    pixels  = np.uint8( pixels )

    if rgb:
        img     = Image.fromarray( pixels, 'RGB' )

    else:
        img     = Image.fromarray( pixels )
        img     = img.convert( 'RGB' )

    return img



def file_to_array( img, rgb ):
    """ -----------------------------------------------------------------------------------------------------
    Load image file as numpy.ndarray

    img:            [str] path to image file
    rgb:            [bool] True if RGB, false if grayscale

    return:         [numpy.ndarray] pixel values (between 0..1)
    ----------------------------------------------------------------------------------------------------- """
    if not isinstance( img, str ):
        raise ValueError( "'{}' should be a string path, not an image object".format( img ) )

    try:
        i   = preprocessing.image.load_img(
                img,
                color_mode   = 'rgb' if rgb else 'grayscale',
                target_size = cnfg[ 'img_size' ]
        )
    except Exception as e:
        raise e

    i       = preprocessing.image.img_to_array( i )
    i       = np.expand_dims( i, axis=0 )
    i       /= 255.                                         # NOTE normalization between 0..1

    return i



def latent_to_image( latent, dec, dec_obj=None, segm=False ):
    """ -----------------------------------------------------------------------------------------------------
    Given a latent space and a decoder model, compute an output image

    latent:         [tf.Tensor or np.ndarray] input latent space
    dec:            [keras.engine.training.Model or list of them] decoder model(s)
    dec_obj:        [arch.MultipleVAE] required only in the case of multiple decoders
    segm:           [bool] if True return segmentations separately

    return:         [PIL.Image.Image] output image or [list of numpy.array]
    ----------------------------------------------------------------------------------------------------- """
    if isinstance( latent, np.ndarray ):
        if len( latent.shape ) == 1:
            latent      = np.expand_dims( latent, axis=0 )      # add batch dimension
        latent      = K.variable( latent )
    
    # case of single decoder
    if not isinstance( dec, ( list, tuple ) ):
        pred        = dec.predict( latent )                            
        img         = array_to_image( pred, out_rgb( decoder ) )

    # case of multiple decoders
    else:
        dec_rgb, dec_car, dec_lane      = dec
        lr, lc, ll                      = dec_obj.split_latent( latent, split=dec_obj.split )
        lat_rgb                         = K.eval( lr )
        lat_car                         = K.eval( lc )
        lat_lane                        = K.eval( ll )

        img_rgb     = dec_rgb.predict( lat_rgb )
        img_car     = dec_car.predict( lat_car )
        img_lane    = dec_lane.predict( lat_lane )
        img         = img_overlays( img_rgb, img_car, img_lane )

        if segm:
            return img_rgb, img_car, img_lane

    return img



def code_to_latent( code ):
    """ -----------------------------------------------------------------------------------------------------
    Given a code (like 'S02ST_000703_RGB_FR') return the corresponding latent representation
    in the current dataset of latent variables, already loaded in the global dset_code_latent, or
    to be load from the file with filename cnfg[ 'decod' ]

    code:           [str] code of frame (like 'S02ST_000703_RGB_FR')

    return          [numpy.array] latent space
    ----------------------------------------------------------------------------------------------------- """
    global dset_code_latent

    dset    = os.path.join( dir_latent, cnfg[ 'decod' ].split( '/' )[ -1 ] + '.h5' )

    if dset_code_latent is None:
        dset_code_latent    = deepdish.io.load( dset )

    if code not in dset_code_latent:
        raise Exception( "code {} not found in {}".format( code, dset ) )

    v   = dset_code_latent[ code ]
    return v.reshape( ( v.shape[ -1 ], ) )       # remove batch size



def code_to_path( code, root='dataset/synthia/scaled_256_128' ):
    """ -----------------------------------------------------------------------------------------------------
    Efficient way to retrieve the full path of a file image, starting from its univoque code,

    code:           [str] image code, like 'S02DW_000028_RGB_FL'
    root:           [str] root dir from where to start the search

    return:         [str] path of image file, starting from the indicated root
    ----------------------------------------------------------------------------------------------------- """
    ext     = '.jpg' if 'RGB' in code else '.png'
    prfx    = code[ :6 ]
    seq     = int( code[ 1:3 ] )

    for s in synthia_names:
        if prfx == s[ 2 ].format( seq ):
            return os.path.join( root, s[ 1 ].format( seq ), code ) + ext

    raise ValueError( "Can't find file corresponding to code {}".format( code ) )



def save_image( img, rgb, fname ):
    """ -----------------------------------------------------------------------------------------------------
    Save an image to file

    img:            [numpy.ndarray or PIL.Image.Image] image
    rgb:            [bool] True if RGB, false if grayscale
    fname:          [str] path of output file
    ----------------------------------------------------------------------------------------------------- """
    if isinstance( img, np.ndarray ):
        img = array_to_image( img, rgb )

    img.save( fname )



def make_collage( imgs, w, h, n_cols=None, n_rows=None, pad_size=5, pad_color="#000000" ):
    """ -----------------------------------------------------------------------------------------------------
    Combine a set of images into a collage

    imgs:           [list of PIL.Image.Image]
    w:              [int] desired width of single image tile inside the collage
    h:              [int] desired height of single image tile inside the collage
    n_cols:         [int] optional number of columns
    n_rows:         [int] optional number of rows
    pad_size:       [int] pixels between image tiles
    pad_color:      [str] padding color

    return:         [PIL.Image.Image]
    ----------------------------------------------------------------------------------------------------- """
    n_imgs  = len( imgs )
    n_cols  = ceil( sqrt( n_imgs ) )    if n_cols is None else n_cols
    n_rows  = ceil( n_imgs / n_cols )   if n_rows is None else n_rows

    width   = n_cols * w + ( n_cols - 1 ) * pad_size
    height  = n_rows * h + ( n_rows - 1 ) * pad_size

    i       = 0
    img     = Image.new( 'RGB', ( width, height ), color=pad_color )
    
    for r in range( n_rows ):
        y   = r * ( h + pad_size )

        for c in range( n_cols ):
            x   = c * ( w + pad_size )

            img.paste( imgs[ i ].resize( ( w, h ) ), ( x, y ) )
            i   += 1

            if i >= n_imgs: 
                break

        if i >= n_imgs: 
            break

    return img
    
    

def save_collage( imgs, w, h, fname, n_cols=None, n_rows=None, pad_size=5, pad_color="#000000" ):
    """ -----------------------------------------------------------------------------------------------------
    Create and save a collage

    imgs:           [list of PIL.Image.Image]
    w:              [int] desired width of single image tile inside the collage
    h:              [int] desired height of single image tile inside the collage
    fname:          [str] path of output file
    n_cols:         [int] optional number of columns
    n_rows:         [int] optional number of rows
    pad_size:       [int] pixels between image tiles
    pad_color:      [str] padding color
    ----------------------------------------------------------------------------------------------------- """
    img     = make_collage( imgs, w, h, n_cols=n_cols, n_rows=n_rows, pad_size=pad_size, pad_color=pad_color )
    img.save( fname )

    

def save_gif( img_list, fname, duration=100, loop=0 ):
    """ -----------------------------------------------------------------------------------------------------
    Create and save an animated GIF from a sequence of images

    img_list:       [list of PIL.Image.Image]
    fname:          [str] path of output file
    duration:       [int] duration of each frame in milliseconds
    loop:           [int] how many times the animation repeats (0 means infinite loop)
    ----------------------------------------------------------------------------------------------------- """
    img_list[ 0 ].save( fname, save_all=True, append_images=img_list[ 1: ], duration=duration, loop=loop )



def img_overlay( img, ovr, color, alpha ):
    """ -----------------------------------------------------------------------------------------------------
    Put an overlay over an image, according to a mask.
    For segmentation visualization.

    img:            [PIL.Image.Image] background image
    ovr:            [numpy.ndarray] B/W mask defining the overlay
    color:          [str] color of the overlay
    alpha:          [float] transparency value (0..1)

    return:         [PIL.Image.Image] combined image
    ----------------------------------------------------------------------------------------------------- """
    clr     = Image.new( 'RGB', img.size, color )

    msk     = np.where( ovr < 0.5, 0, alpha )
    msk     = array_to_image( msk, False, rng=( alpha * 256 ) )
    msk     = msk.convert( 'L' )

    edg     = msk.filter( ImageFilter.FIND_EDGES )

    out     = Image.composite( clr, img, msk )
    out     = Image.composite( clr, out, edg )
    return out



def img_overlays( img, ovr_c, ovr_l, fname=None ):
    """ -----------------------------------------------------------------------------------------------------
    Given a background image, plot two overlayes corresponding to cars and lanes.
    For segmentation visualization.

    img:            [numpy.ndarray or PIL.Image.Image] background RGB image
    ovr_c:          [numpy.ndarray or PIL.Image.Image] CAR mask image
    ovr_l:          [numpy.ndarray or PIL.Image.Image] LANE mask image
    fname:          [str] filename to save output

    return:         [PIL.Image.Image] combined image
    ----------------------------------------------------------------------------------------------------- """
    if isinstance( img, np.ndarray ):
        img = array_to_image( img, True )
    if not isinstance( ovr_c, np.ndarray ):
        ovr_c = np.array( ovr_c )               # it does not work with 'PngImageFile'
    if not isinstance( ovr_l, np.ndarray ):
        ovr_l = np.array( ovr_l )               # it does not work with 'PngImageFile'

    o   = img_overlay( img, ovr_l, lane_color, alpha_overlay )
    o   = img_overlay( o, ovr_c, car_color, alpha_overlay )

    if fname is not None:
        o.save( fname )

    return o



def get_targ( fname, segm=False ):
    """ -----------------------------------------------------------------------------------------------------
    Given a file of a RGB image, return its combination with the segmented car/lane

    fname:          [str] path to input RGB image file
    segm:           [bool] if True return segmentations separately

    return:         [PIL.Image.Image] target image with overlays or [list of numpy.array]
    ----------------------------------------------------------------------------------------------------- """
    if not os.path.isfile( fname ):
        ms.print_err( "File '{}' not found".format( fname ) )

    if 'RGB' not in fname:
        ms.print_err( "Input file '{}' is not an RGB image".format( fname ) )

    if not fname.endswith( '.jpg' ):
        ms.print_err( "Input image '{}' is not a JPEG file".format( fname ) )

    c   = fname.replace( 'RGB', 'SEGM_CAR' ).replace( '.jpg', '.png' )
    l   = fname.replace( 'RGB', 'SEGM_LANE' ).replace( '.jpg', '.png' )

    ir  = file_to_array( fname, True )
    ic  = file_to_array( c, False )
    il  = file_to_array( l, False )

    if segm:
        return ir, ic, il

    return img_overlays( ir, ic, il )



# ===========================================================================================================
#
#   Predictions from model
#
#   - pred_image
#   - pred_multi_image
#   - pred_multirec_image
#
#   - pred_folder
#   - pred_tset
#   - pred_multi_tset
#   - pred_rec_tset
#   - pred_multirec_tset
#
#   - pred_time
#   - pred_time_sample
#   - hallucinate
#
#   - interpolate_pred
#   - swap_latent
#
# ===========================================================================================================

def pred_image( model, inp, targ=None, save_p=None, save_i=None, save_t=None ):
    """ -----------------------------------------------------------------------------------------------------
    Return the image prediction, given an input image, for a model with single input and single output.
    Optionally save the prediction, the input and the target image

    model:          [keras.engine.training.Model]
    inp:            [str] pathname of input image file
    targ:           [str] pathname of target image file
    save_p:         [str] filename to save predicted image
    save_i:         [str] filename to save input image
    save_t:         [str] filename to save target image

    return:         [numpy.ndarray] prediction
    ----------------------------------------------------------------------------------------------------- """
    i       = file_to_array( inp, in_rgb( model ) )
    pred    = model.predict( i )

    if isinstance( save_p, str ):
        save_image( pred, out_rgb( model ), save_p )

    if isinstance( save_i, str ):
        save_image( i, in_rgb( model ), save_i )

    if isinstance( save_t, str ) and targ is not None:
        save_image( targ, out_rgb( model ), save_t )

    return pred



def pred_multi_image( model, inp, save_p=None, save_i=None ):
    """ -----------------------------------------------------------------------------------------------------
    Return the image prediction, given an input image, for a model with single input and segmented output.
    Optionally save the combined prediction, the input and the target image

    model:          [keras.engine.training.Model] model with multiple outputs (MultiVAE)
    inp:            [str] pathname of input image file
    save_p:         [bool or str] False or filename to save combined predicted image
    save_i:         [bool or str] False or filename to save input image

    return:         [list of numpy.ndarray] predictions of RGB, CAR, LANE and [PIL.Image.Image] combined
    ----------------------------------------------------------------------------------------------------- """
    i                               = file_to_array( inp, in_rgb( model ) )
    
    # NOTE it supposes that the order of MVAE loss values is ( RGB, CAR, LANE )
    pred_rgb, pred_car, pred_lane   = model.predict( i )
    pred                            = img_overlays( pred_rgb, pred_car, pred_lane )

    if isinstance( save_p, str ):
        save_image( pred, True, save_p )

    if isinstance( save_i, str ):
        save_image( i, in_rgb( model ), save_i )

    return pred_rgb, pred_car, pred_lane, pred



def pred_multirec_image( model, inp, save_p=None, save_i=None ):
    """ -----------------------------------------------------------------------------------------------------
    Return the image predictions, given an input image, for a model with multi input and
    multi-segmented output.
    Optionally save the combined prediction, the input and the target image

    model:          [keras.engine.training.Model] model with multiple input/outputs (RecMultiVAE)
    inp:            [list of str] pathname of input image files
    save_p:         [bool or str] False or folder to save combined predicted image
    save_i:         [bool or str] False or folder to save input image

    return:         [list of PIL.Image] overlayed predictions
    ----------------------------------------------------------------------------------------------------- """
    list_i      = [ file_to_array( i, in_rgb( model ) ) for i in inp ]
    
    # NOTE it supposes that the order of RMVAE loss values is ( RGB, CAR, LANE )
    list_p      = model.predict( list_i )
    list_r      = list_p[ ::3 ]             # only rgbs
    list_c      = list_p[ 1::3 ]            # only cars
    list_l      = list_p[ 2::3 ]            # only lanes

    pred        = []
    for n in range( len( list_p ) // 3 ):
        pred.append( img_overlays( list_r[ n ], list_c[ n ], list_l[ n ] ) )

    if isinstance( save_p, str ):
        if not os.path.isdir( save_p ):
            os.makedirs( save_p )
        for n, p in enumerate( pred ):
            save_image( p, True, save_p + "/p_{}.png".format( n ) )

    if isinstance( save_i, str ):
        if not os.path.isdir( save_i ):
            os.makedirs( save_i )
        for n, i in enumerate( list_i ):
            save_image( i, in_rgb( model ), save_i + "/i_{}.jpg".format( n ) )

    return pred



def pred_folder( model, input_dir, save_dir, multi_output=False, segm=True ):
    """ -----------------------------------------------------------------------------------------------------
    Compute the image predictions, over a folder of ordered images. Then save the predicted images.

    model:          [keras.engine.training.Model]
    input_dir:      [str] folder of input images
    save_dir:       [str] path where to save new images
    multi_output:   [bool] True if the kind of model is MVAE-like
    ----------------------------------------------------------------------------------------------------- """
    if not os.path.isdir( input_dir ):
        raise ValueError( "{} directory does not exist".format( input_dir ) )

    if not os.path.isdir( save_dir ):
        os.makedirs( save_dir )

    imgs    = [ f for f in sorted( os.listdir( input_dir ) ) if f.endswith( '.jpg' ) ]

    for i in imgs:
        if multi_output:
            r       = os.path.join( input_dir, i )
            o       = os.path.join( save_dir, i.replace( '.jpg', '.png' ) )
            pred_multi_image( model, r, save_p=o )
            if segm:
                # FIXME use new function get_targ()
                c   = r.replace( 'RGB', 'SEGM_CAR' ).replace( '.jpg', '.png' )
                l   = r.replace( 'RGB', 'SEGM_LANE' ).replace( '.jpg', '.png' )
                ii  = img_overlays( Image.open( r ), Image.open( c ), Image.open( l ) )
                ii.save( o.replace( '.', '_targ.' ) )
        else:
            pred_image( model, i, save_p=os.path.join( save_dir, i ) )

    

def pred_tset( model, input_dir, target_dir, save_dir, samples=16 ):
    """ -----------------------------------------------------------------------------------------------------
    Compute the image predictions of a simple model (single input, single output),
    over a sample of test images. Plot a collage of the predicted images compared with the target images.

    model:          [keras.engine.training.Model]
    input_dir:      [str] folder of input images
    target_dir:     [str] folder of target images
    save_dir:       [str] path where to save plots
    samples:        [int] number of samples of test set to use (if 0 use all)
    ----------------------------------------------------------------------------------------------------- """
    if not os.path.isdir( input_dir ):
        raise ValueError( "{} directory does not exist".format( input_dir ) )

    if not os.path.isdir( target_dir ):
        raise ValueError( "{} directory does not exist".format( target_dir ) )

    # file names
    i_files     = np.array( sorted( [ os.path.join( input_dir, f )
            for f in os.listdir( input_dir ) if cond_ext( f.lower() ) ] ) )
    t_files    = np.array( sorted( [ os.path.join( target_dir, f )
            for f in os.listdir( target_dir ) if cond_ext( f.lower() ) ] ) )

    # get sample indices
    if samples > 0 and samples < len( i_files ):
        sample_indx     = np.linspace( 0, len( i_files ) - 1, num=samples, endpoint=True, dtype=int )
    else:
        sample_indx     = list( range( len( i_files ) ) )

    # get names of all image files of the samples
    i_samples   = i_files[ sample_indx ]
    t_samples   = t_files[ sample_indx ]

    # predicted images in array format
    o_rgb       = out_rgb( model )
    outputs     = [ pred_image( model, i ) for i in i_samples ]
    outputs     = [ array_to_image( o, o_rgb ) for o in outputs ]               # predicted images in PIL
    targets     = [ Image.open( i ) for i in t_samples ]                        # target images

    h, w        = imgsize( model )
    save_collage( targets, w, h, os.path.join( save_dir, "smpl_target.png" ) )
    save_collage( outputs, w, h, os.path.join( save_dir, "smpl_predict.png" ) )

    

def pred_multi_tset( model, input_dir, target_dirs, save_dir, samples=16, raw=True ):
    """ -----------------------------------------------------------------------------------------------------
    Compute the image predictions of a MultiVAE model (single input, multi-segmented output),
    over a sample of test images. Plot a collage of the predicted images compared with the target images.

    The segmented predictions of different classes are combined together in a single image.
    But optionally can produce the 'raw' triplets of images.

    model:          [keras.engine.training.Model]
    input_dir:      [str] folder of input images
    target_dir:     [list of str] multiple folders of target images
    save_dir:       [str] path where to save plots
    samples:        [int] number of samples of test set to use (if 0 use all)
    raw:            [bool] save separated prediction for the different multi classes
    ----------------------------------------------------------------------------------------------------- """
    if not os.path.isdir( input_dir ):
        raise ValueError( "{} directory does not exist".format( input_dir ) )

    for target_dir in target_dirs:
        if not os.path.isdir( target_dir ):
            raise ValueError( "{} directory does not exist".format( target_dir ) )

    # file names
    i_files     = np.array( sorted( [ os.path.join( input_dir, f )
            for f in os.listdir( input_dir ) if cond_ext( f.lower() ) ] ) )

    t_files     = []
    for target_dir in target_dirs:
        t_files.append( np.array( sorted( [ os.path.join( target_dir, f )
                for f in os.listdir( target_dir ) if cond_ext( f.lower() ) ] ) )
        )

    # get sample indices
    if samples > 0 and samples < len( i_files ):
        sample_indx     = np.linspace( 0, len( i_files ) - 1, num=samples, endpoint=True, dtype=int )
    else:
        sample_indx     = list( range( len( i_files ) ) )

    # get names of all image files of the samples
    i_samples   = i_files[ sample_indx ]
    t_samples   = []
    for t_file in t_files:
        t_samples.append( t_file[ sample_indx ] )

    # predicted images in array format
    outputs         = [ pred_multi_image( model, i ) for i in i_samples ]
    outputs_r       = [ o[ 0 ] for o in outputs ]
    outputs_c       = [ o[ 1 ] for o in outputs ]
    outputs_l       = [ o[ 2 ] for o in outputs ]
    outputs_over    = [ o[ 3 ] for o in outputs ]

    # input images
    inputs      = []
    for i in i_samples:
        inputs.append( Image.open( i ) )

    # target images
    targets     = []
    for t_sample in t_samples:
        targets.append( [ Image.open( i ) for i in t_sample ] )
    targets_over    = [ img_overlays( r, c, l ) for r, c, l in zip( *targets ) ] 

    h, w        = imgsize( model )
    save_collage( outputs_over, w, h, os.path.join( save_dir, "smpl_predict.png" ) )
    save_collage( targets_over, w, h, os.path.join( save_dir, "smpl_target.png" ) )
    save_collage( inputs, w, h, os.path.join( save_dir, "smpl_input.png" ) )

    # optionally save separated triplets of images for RGB, CAR, LANE
    if raw:
        outputs_r       = [ array_to_image( o, True ) for o in outputs_r ]
        outputs_c       = [ array_to_image( o, False ) for o in outputs_c ]
        outputs_l       = [ array_to_image( o, False ) for o in outputs_l ]
        save_collage( outputs_r, w, h, os.path.join( save_dir, "smpl_predict_rgb.png" ) )
        save_collage( outputs_c, w, h, os.path.join( save_dir, "smpl_predict_car.png" ) )
        save_collage( outputs_l, w, h, os.path.join( save_dir, "smpl_predict_lane.png" ) )

        save_collage( targets[ 0 ], w, h, os.path.join( save_dir, "smpl_target_rgb.png" ) )
        save_collage( targets[ 1 ], w, h, os.path.join( save_dir, "smpl_target_car.png" ) )
        save_collage( targets[ 2 ], w, h, os.path.join( save_dir, "smpl_target_lane.png" ) )

    

# TODO the separation between inp_imgs and out_imgs is useless, should be fix like in 'pred_multirec_tset'
def pred_rec_tset( model, tset_dir, save_dir, samples=16 ):
    """ -----------------------------------------------------------------------------------------------------
    Compute the image predictions of a RecursiveVAE model (multiple inputs, multiple outputs),
    over a sample of test images. Plot a collage of the predicted images compared with the target images.

    model:          [keras.engine.training.Model]
    tset_dir:       [str] folder of input images
    save_dir:       [str] path where to save plots
    samples:        [int] number of samples of test set to use (if 0 use all)
    ----------------------------------------------------------------------------------------------------- """
    if not os.path.isdir( tset_dir ):
        raise ValueError( "{} directory does not exist".format( tset_dir ) )

    n_inp   = cnfg[ 'n_input' ]
    n_out   = cnfg[ 'n_output' ]

    dirs    = [ os.path.join( tset_dir, d, 'img' ) for d in sorted( os.listdir( tset_dir ) )
                        if os.path.isdir( os.path.join( tset_dir, d ) ) ]

    # file names
    files   = []
    for d in dirs:
        files.append( np.array( sorted( [ os.path.join( d, f )
                for f in os.listdir( d ) if cond_ext( f.lower() ) ] ) ) )

    # get sample indices
    if samples > 0 and samples < len( files[ 0 ] ):
        sample_indx     = np.linspace( 0, len( files[ 0 ] ) - 1, num=samples, endpoint=True, dtype=int )
    else:
        sample_indx     = list( range( len( files[ 0 ] ) ) )

    # get names of all image files of the samples
    f_samples   = []
    for f in files:
        f_samples.append( f[ sample_indx ] )

    # convert files into images and arrays, for input and output separately (for target visualization)
    inp_arrs        = []
    inp_imgs        = []
    out_imgs        = []
    for v in zip( *f_samples ):
        inp_arrs.append( [ file_to_array( i, True ) for i in v[ : n_inp ] ] )
        inp_imgs.append( [ Image.open( i ) for i in v[ : n_inp ] ] )
        out_imgs.append( [ Image.open( i ) for i in v[ - n_out : ] ] )

    # predicted images
    pred_arrs       = [ model.predict( i ) for i in inp_arrs ]
    pred_imgs       = []
    for v in pred_arrs:
        for w in v:
            pred_imgs.append( array_to_image( w, True ) )

    w, h            = pred_imgs[ 0 ].size
    save_collage( pred_imgs, w, h, os.path.join( save_dir, "smpl_predict.png" ), n_cols=n_inp+n_out )

    # target images
    targs           = []
    for n in range( samples ):
        for i in inp_imgs[ n ]:
            targs.append( i )
        for i in out_imgs[ n ]:
            targs.append( i )
    save_collage( targs, w, h, os.path.join( save_dir, "smpl_target.png" ), n_cols=n_inp+n_out )

    

def pred_multirec_tset( model, tset_dir, save_dir, samples=16 ):
    """ -----------------------------------------------------------------------------------------------------
    Compute the image predictions of a RecMultiVAE model (multiple inputs, multiple segmented outputs),
    over a sample of test images.
    Plot a collage of the predicted images, a plot of the input images, and a plot of the target
    images (segmented).

    model:          [keras.engine.training.Model]
    tset_dir:       [str] folder of input images
    save_dir:       [str] path where to save plots
    samples:        [int] number of samples of test set to use (if 0 use all)
    ----------------------------------------------------------------------------------------------------- """
    if not os.path.isdir( tset_dir ):
        raise ValueError( "{} directory does not exist".format( tset_dir ) )

    # used for plot grid size
    n_inp   = cnfg[ 'n_input' ]
    n_out   = cnfg[ 'n_output' ]

    # subfolders of test set
    dirs_rgb    = [ os.path.join( tset_dir, d, 'rgb/img' ) for d in sorted( os.listdir( tset_dir ) )
                        if os.path.isdir( os.path.join( tset_dir, d ) ) ]
    dirs_car    = [ os.path.join( tset_dir, d, 'car/img' ) for d in sorted( os.listdir( tset_dir ) )
                        if os.path.isdir( os.path.join( tset_dir, d ) ) ]
    dirs_lane   = [ os.path.join( tset_dir, d, 'lane/img' ) for d in sorted( os.listdir( tset_dir ) )
                        if os.path.isdir( os.path.join( tset_dir, d ) ) ]

    # file names separated for RGB/CAR/LANE
    files_rgb   = []
    files_car   = []
    files_lane  = []

    for d in dirs_rgb:
        files_rgb.append( np.array( sorted( [ os.path.join( d, f )
                for f in os.listdir( d ) if cond_ext( f.lower() ) ] ) ) )
    for d in dirs_car:
        files_car.append( np.array( sorted( [ os.path.join( d, f )
                for f in os.listdir( d ) if cond_ext( f.lower() ) ] ) ) )
    for d in dirs_lane:
        files_lane.append( np.array( sorted( [ os.path.join( d, f )
                for f in os.listdir( d ) if cond_ext( f.lower() ) ] ) ) )

    # get sample indices
    if samples > 0 and samples < len( files_rgb[ 0 ] ):
        sample_indx     = np.linspace( 0, len( files_rgb[ 0 ] ) - 1, num=samples, endpoint=True, dtype=int )
    else:
        sample_indx     = list( range( len( files_rgb[ 0 ] ) ) )

    # get names of all image files of the samples separated for RGB/CAR/LANE
    fr_samples  = []
    fc_samples  = []
    fl_samples  = []

    for f in files_rgb:
        fr_samples.append( f[ sample_indx ] )
    for f in files_car:
        fc_samples.append( f[ sample_indx ] )
    for f in files_lane:
        fl_samples.append( f[ sample_indx ] )

    # get arrays from files, separated for RGB/CAR/LANE, and grouped in tuple for each input block
    inp_files       = []
    inp_imgs_r      = []
    inp_imgs_c      = []
    inp_imgs_l      = []

    for v in zip( *fr_samples ):
        inp_files.append( [ i for i in v[ : n_inp ] ] )                         # input for prediction
        inp_imgs_r.append( [ Image.open( i ) for i in v ] )                     # for visualization
    for v in zip( *fc_samples ):
        inp_imgs_c.append( [ file_to_array( i, False ) for i in v ] )           # for visualization
    for v in zip( *fl_samples ):
        inp_imgs_l.append( [ file_to_array( i, False ) for i in v ] )           # for visualization

    # predicted images
    pred_imgs       = [ pred_multirec_image( model, i ) for i in inp_files ]
    pred_imgs_flat  = [ item for sublist in pred_imgs for item in sublist ]     # flat list of sublists
    w, h            = pred_imgs_flat[ 0 ].size
    save_collage( pred_imgs_flat, w, h, os.path.join( save_dir, "smpl_predict.png" ), n_cols=n_inp+n_out )

    # input images
    inputs          = [ item for sublist in inp_imgs_r for item in sublist ]     # flat list of sublists
    save_collage( inputs, w, h, os.path.join( save_dir, "smpl_input.png" ), n_cols=n_inp+n_out )

    # target images (that is input images with segmented overlay)
    targets         = []
    for rr, cc, ll in zip( inp_imgs_r, inp_imgs_c, inp_imgs_l ):
        for r, c, l in zip( rr, cc, ll ):
            targets.append( img_overlays( r, c, l ) )
    save_collage( targets, w, h, os.path.join( save_dir, "smpl_target.png" ), n_cols=n_inp+n_out )

    

def pred_time( model, decoder, test_set, test_str, save_dir, data_dir, dec_obj=None, odom=True, samples=50, prfx='' ):
    """ -----------------------------------------------------------------------------------------------------
    Convert to image space the predicions in feature space over time.
    Produce collages of predictions, inputs and outputs.
    Generate predictions for all the test set.

    see NOTE at trainer.train_time_model() for the difference in format between prediction with
    multiple Dense and prediction with recursion

    NOTE also that the difference in format is for test_set, while test_str is maintained with
    multiple Dense format in any case

    model:          [keras.engine.training.Model] RNN for time prediction in feature space
    decoder:        [keras.engine.training.Model or list of them] *AE decoding from feature space to image space
    test_set:       [tuple] (x,y) for testing the RNN
    test_str:       [tuple] (x,y) same structure of test_set (without odometry) containing string names
    save_dir:       [str] path where to save plots
    data_dir:       [str] dataset containing all frame images
    dec_obj:        [arch.MultipleVAE] required only in the case of multiple decoders
    odom:           [bool] True if dataset contains odometries
    samples:        [int] number of samples of test set to use (if 0 use all)
    ----------------------------------------------------------------------------------------------------- """
    if isinstance( decoder, ( tuple, list ) ):
        o_rgb       = True
    else:
        o_rgb       = out_rgb( decoder )

    # get vectors of input latent spaces
    if odom:
        # also get vectors of input odometry
        n_odom      = ( len( test_set[ 0 ] ) - 1 ) // 2
        i_odom      = test_set[ 0 ][ : n_odom ]     # input odometry
        i_frms      = test_set[ 0 ][ n_odom : ]     # input latent frames
        assert len( i_odom[ 0 ] ) == len( i_frms[ 0 ] )
        assert len( i_odom ) + 1 == len( i_frms )
    else:
        i_frms      = test_set[ 0 ]                 # input latent frames

    t_frms      = test_set[ 1 ]                     # vectors of target latent spaces
    i_strs      = test_str[ 0 ]                     # names of input images generating the latent spaces
    t_strs      = test_str[ 1 ]                     # names of target images generating the latent spaces

    # get number of elements in the dataset, as couple of inp-out (tot)
    # get number of inputs for each sample (n_inp)
    # get number of outputs for each sample (n_out)
    if odom:
        assert len( i_frms[ 0 ] ) == len( t_frms[ 0 ] )
        tot         = len( i_frms[ 0 ] )
        n_inp       = len( i_frms )
        n_out       = len( t_frms )
    else:                                           # case of multiple outputs
        if isinstance( t_frms, ( tuple, list ) ):
            assert i_frms.shape[ 0 ] == len( t_frms[ 0 ] )
            tot         = len( t_frms[ 0 ] )
            n_inp       = i_frms.shape[ 1 ]
            n_out       = len( t_frms )
        else:                                       # case of single output
            assert i_frms.shape[ 0 ] == t_frms.shape[ 0 ]
            tot         = i_frms.shape[ 0 ]
            n_inp       = i_frms.shape[ 1 ]
            n_out       = 1

    # get sample indices
    if samples > 0 and samples < tot:
        sample_indx     = np.linspace( 0, tot - 1, num=samples, endpoint=True, dtype=int )

        # get samples from dataset
        if odom:
            i_odom      = np.array( [ f[ sample_indx ] for f in i_odom ] )
            i_frms      = np.array( [ f[ sample_indx ] for f in i_frms ] )
        else:
            i_frms      = i_frms[ sample_indx ]

        i_strs      = np.array( [ f[ sample_indx ] for f in i_strs ] )
        t_strs      = np.array( [ f[ sample_indx ] for f in t_strs ] )

    i_strs      = np.array( i_strs ).swapaxes( 0, 1 )
    t_strs      = np.array( t_strs ).swapaxes( 0, 1 )

    # predicted latent frames
    if cnfg[ 'arch_class' ] == 'TIME':
        o_frms      = model.predict( [ *i_odom, *i_frms ] )
    elif cnfg[ 'arch_class' ] in ( 'RTIME', 'R2TIME', 'RMTIME' ):
        o_frms      = model.predict( i_frms )
    elif cnfg[ 'arch_class' ] == 'ITIME':
        o_frms      = model.predict( i_frms )
    else:
        ms.print_err( "Arch code {} not valid".format( cnfg[ 'arch_class' ] ) )

    # convert predicted latent frames to images
    o_imgs      = [ [] for i in range( n_out ) ]

    # the case of just one predicted frame needs specific coding, because the output list has one less level
    if n_out == 1:
        for l in o_frms:
            o_imgs[ 0 ].append( latent_to_image( l, decoder, dec_obj=dec_obj ) )
    else:
        for i in range( n_out ):
            for l in o_frms[ i ]:
                o_imgs[ i ].append( latent_to_image( l, decoder, dec_obj=dec_obj ) )

    o_imgs          = list( map( list, zip( *o_imgs ) ) )                   # swapping axes without numpy
    o_imgs          = [ item for sublist in o_imgs for item in sublist ]    # flat list of sublists

    # get image files corresponding to input latent spaces
    i_files     = [ [] for i in range( samples ) ]
    for i in range( samples ):
        for j in i_strs[ i ]:
            i_files[ i ].append( glob.glob( '{}/**/{}.*'.format( data_dir, j ), recursive=True ) )
    i_files     = [ i for l in sum( i_files, [] ) for i in l ]              # flat list of sub-sublists
    i_imgs      = [ Image.open( i ) for i in i_files ]

    # get image files corresponding to target latent spaces
    t_files     = [ [] for i in range( samples ) ]
    for i in range( samples ):
        for j in t_strs[ i ]:
            t_files[ i ].append( glob.glob( '{}/**/{}.*'.format( data_dir, j ), recursive=True ) )
    t_files     = [ i for l in sum( t_files, [] ) for i in l ]              # flat list of sub-sublists
    t_imgs      = [ Image.open( i ) for i in t_files ]

    # in case of segmented output, visualize the target images with overlays
    if isinstance( decoder, ( tuple, list ) ):
        # FIXME use new function get_targ()
        t_files_c   = [ s.replace( 'RGB', 'SEGM_CAR' ) for s in t_files ]
        t_files_l   = [ s.replace( 'RGB', 'SEGM_LANE' ) for s in t_files ]
        t_files_c   = [ s.replace( '.jpg', '.png' ) for s in t_files_c ]
        t_files_l   = [ s.replace( '.jpg', '.png' ) for s in t_files_l ]
        t_imgs_c    = [ Image.open( i ) for i in t_files_c ]
        t_imgs_l    = [ Image.open( i ) for i in t_files_l ]
        t_imgs      = [ img_overlays( r, c, l ) for r, c, l in zip( t_imgs, t_imgs_c, t_imgs_l ) ]

    if isinstance( decoder, ( tuple, list ) ):
        h, w        = imgsize( decoder[ 0 ], inp=False )
    else:
        h, w        = imgsize( decoder, inp=False )

    prfx        += 'smpl'

    save_collage( i_imgs, w, h, os.path.join( save_dir, "{}_input.png".format( prfx ) ), n_cols=n_inp )
    save_collage( t_imgs, w, h, os.path.join( save_dir, "{}_target.png".format( prfx ) ), n_cols=n_out )
    save_collage( o_imgs, w, h, os.path.join( save_dir, "{}_predict.png".format( prfx ) ), n_cols=n_out )



def pred_time_sample( model, dec, dec_obj, data_dir, first_img, save_dir, suffx='', gif=True ):
    """ -----------------------------------------------------------------------------------------------------
    Produce a future prediction, given a single input sample

    model:          [keras.engine.training.Model] RNN for time prediction in feature space
    dec:            [keras.engine.training.Model or list of them] *AE decoding from feature space to image space
    dec_obj:        [arch.MultipleVAE] required only in the case of multiple decoders
    data_dir:       [str] dataset containing all frame images
    first_img:      [str] code of first input image (like 'S02ST_000703_RGB_FR')
    save_dir:       [str] path where to save plots
    suffx:          [str] optional suffix for output file
    gif:            [bool] if True save animation as gif, otherwise as images
    ----------------------------------------------------------------------------------------------------- """
    n_inp           = cnfg[ 'n_input' ]
    n_out           = cnfg[ 'n_output' ]
    step            = cnfg[ 'step' ]

    inp_code        = []                                # list of inputs as string codes (like 'S02ST_000703_RGB_FR')
    out_code        = []                                # list of targets as string codes
    inp_lat         = []                                # list of inputs as latent spaces
    inp_img         = []                                # list of inputs as PIL images

    # fill the list of inputs as string codes
    frame_indx      = int( first_img[ 6 : 12 ] )        # index of first input frame
    for _ in range( n_inp ):
        s           = first_img[ :6 ] + '{:06d}'.format( frame_indx ) + first_img[ 12: ]
        inp_code.append( s )
        frame_indx  += step
    for _ in range( n_out ):
        s           = first_img[ :6 ] + '{:06d}'.format( frame_indx ) + first_img[ 12: ]
        out_code.append( s )
        frame_indx  += step

    # fill the list of inputs as latent spaces and vectors
    inp_lat     += list( map( code_to_latent, inp_code ) )
    inp_arr     = np.array( inp_lat[ -n_inp : ] )
    inp_arr     = np.expand_dims( inp_arr, axis=0 )     # add batch dimension

    # predict the future frames
    out_arr     = model.predict( inp_arr )
    out_lat     = [ o[ 0, : ] for o in out_arr ]        # remove batch dimension
    out_img     = [ latent_to_image( o, dec, dec_obj=dec_obj ) for o in out_arr ]

    # retireve the corresponding target images
    img_r       = []
    img_c       = []
    img_l       = []
    for s in out_code:
        img_r.append( Image.open( os.path.join( data_dir, s + '.jpg' ) ) )
        # FIXME use new function get_targ()
        img_c.append( Image.open( os.path.join( data_dir, s.replace( 'RGB', 'SEGM_CAR' ) + '.png' ) ) )
        img_l.append( Image.open( os.path.join( data_dir, s.replace( 'RGB', 'SEGM_LANE' ) + '.png' ) ) )
    targ_img    = [ img_overlays( r, c, l ) for r, c, l in zip( img_r, img_c, img_l ) ]

    # retireve the input images both segmented and plain rgb
    img_r       = []
    img_c       = []
    img_l       = []
    for s in inp_code:
        img_r.append( Image.open( os.path.join( data_dir, s + '.jpg' ) ) )
        # FIXME use new function get_targ()
        img_c.append( Image.open( os.path.join( data_dir, s.replace( 'RGB', 'SEGM_CAR' ) + '.png' ) ) )
        img_l.append( Image.open( os.path.join( data_dir, s.replace( 'RGB', 'SEGM_LANE' ) + '.png' ) ) )
    inp_img     = [ img_overlays( r, c, l ) for r, c, l in zip( img_r, img_c, img_l ) ]

    h, w        = imgsize( dec[ 0 ], inp=False )
    if not os.path.isdir( save_dir ):
        os.makedirs( save_dir )

    l1          = inp_img + out_img
    l2          = inp_img + targ_img

    # save results as animated gif
    if gif:
        list_full   = list( zip( l1, l2 ) )
        img_full    = [ make_collage( i, w, h, pad_color="#FFFFFF", n_cols=1 ) for i in list_full ]
        save_gif( img_full, os.path.join( save_dir, "timepred{}.gif".format( suffx ) ), duration=300 )

    # save results as collages of frames
    else:
        save_collage( out_img, w, h, os.path.join( save_dir, "time_pred{}.png".format( suffx ) ) )
        save_collage( targ_img, w, h, os.path.join( save_dir, "time_targ{}.png".format( suffx ) ) )
        for i in range( len( l1 ) ):
            l1[ i ].save( os.path.join( save_dir, "time_pred{}_{}.png".format( suffx, i ) ) )
            l2[ i ].save( os.path.join( save_dir, "time_targ{}_{}.png".format( suffx, i ) ) )



def hallucinate( model, dec, dec_obj, data_dir, first_img, save_dir, use_all=False, n_iter=20, suffx='', gif=True ):
    """ -----------------------------------------------------------------------------------------------------
    Prediction in time using the hallucination method: the output at iteration 'n' is used as input for
    iteration 'n+1'.

    model:          [keras.engine.training.Model] RNN for time prediction in feature space
    dec:            [keras.engine.training.Model or list of them] *AE decoding from feature space to image space
    dec_obj:        [arch.MultipleVAE] required only in the case of multiple decoders
    data_dir:       [str] dataset containing all frame images
    first_img:      [str] code of first input image (like 'S02ST_000703_RGB_FR')
    save_dir:       [str] path where to save plots
    use_all:        [bool] if True propagate all outputs of iteration 'n' as input of iter 'n+1';
                           otherwise propagate only the first output
    n_iter:         [int] number of iterations
    suffx:          [str] optional suffix for output file
    gif:            [bool] if True save animation as gif, otherwise as images
    ----------------------------------------------------------------------------------------------------- """
    n_inp           = cnfg[ 'n_input' ]
    n_out           = cnfg[ 'n_output' ]
    step            = cnfg[ 'step' ]

    list_code       = []                                # list of frames as string codes (like 'S02ST_000703_RGB_FR')
    list_lat        = []                                # list of frames as latent spaces
    list_img        = []                                # list of frames as PIL images
    frame_indx      = int( first_img[ 6 : 12 ] )        # index of most recent frame in the list

    def update_frame( frm_i, n ):
        """ -------------------------------------------------------------------------------------------------
        Update the list of codes of frames.

        frm_i:      [int] index of most recent frame in the list
        n:          [int] number of new input to add to the list

        return:     [int] updated index of most recent frame in the list
        ------------------------------------------------------------------------------------------------- """
        for _ in range( n ):
            frm_i       += step
            s           = first_img[ :6 ] + '{:06d}'.format( frm_i ) + first_img[ 12: ]
            list_code.append( s )
        return frm_i


    # initialize the list of frames with a sequence of 'n_inp' frames taken from the dataset
    # the first input of this sequence is passed as argument
    list_code.append( first_img )
    frame_indx      = update_frame( frame_indx, n_inp - 1 )
    list_lat        += list( map( code_to_latent, list_code ) )

    for _ in range( n_iter ):

        # at each iteration use the last 'n_inp' frames in the list as input for the model
        inps        = np.array( list_lat[ -n_inp : ] )
        inps        = np.expand_dims( inps, axis=0 )        # add batch dimension
        outs        = model.predict( inps )

        # then append the desired number of outputs (1 or all) to the list of frames
        # in this way, they will be used as new input for the next iteration
        propagate   = outs if use_all else [ outs[ 0 ], ]
        for o in propagate:
            list_lat.append( o[ 0, : ] )                    # remove batch dimension
            list_img.append( latent_to_image( o, dec, dec_obj=dec_obj ) )

        frame_indx  = update_frame( frame_indx, len( propagate ) )

    # retireve the corresponding target images
    img_r       = []
    img_c       = []
    img_l       = []
    for s in list_code[ n_inp: ]:
        img_r.append( Image.open( os.path.join( data_dir, s + '.jpg' ) ) )
        # FIXME use new function get_targ()
        img_c.append( Image.open( os.path.join( data_dir, s.replace( 'RGB', 'SEGM_CAR' ) + '.png' ) ) )
        img_l.append( Image.open( os.path.join( data_dir, s.replace( 'RGB', 'SEGM_LANE' ) + '.png' ) ) )
    targ_img    = [ img_overlays( r, c, l ) for r, c, l in zip( img_r, img_c, img_l ) ]

    h, w        = imgsize( dec[ 0 ], inp=False )
    if not os.path.isdir( save_dir ):
        os.makedirs( save_dir )

    # save results as animated gif
    if gif:
        list_couple = list( zip( list_img, targ_img ) )
        img_couple  = [ make_collage( c, w, h, pad_color="#FFFFFF" ) for c in list_couple ]
        save_gif( img_couple, os.path.join( save_dir, "halluc{}.gif".format( suffx ) ), duration=150 )

    # save results as collages of frames
    else:
        save_collage( list_img, w, h, os.path.join( save_dir, "halluc_pred{}.png".format( suffx ) ) )
        save_collage( targ_img, w, h, os.path.join( save_dir, "halluc_targ{}.png".format( suffx ) ) )
        for i in range( len( list_img ) ):
            list_img[ i ].save( os.path.join( save_dir, "halluc_pred{}_{}.png".format( suffx, i ) ) )
            targ_img[ i ].save( os.path.join( save_dir, "halluc_targ{}_{}.png".format( suffx, i ) ) )



def interpolate_pred( encoder, decoder, img_1, img_2, model_obj=None, steps=14, gif=True, save='interp.png', suffx='' ):
    """ -----------------------------------------------------------------------------------------------------
    Generate a set of images predicted from the interpolation between two latent spaces, obtained from
    two different images

    encoder:        [keras.engine.training.Model]
    decoder:        [keras.engine.training.Model or list]
    img_1:          [str] pathname of first input image file
    img_2:          [str] pathname of second input image file
    model_obj:      [arch.MultipleVAE] required only in the case of MultipleVAE prediction
    steps:          [int] number of interpolated images to generate (excluding the two orginals)
    gif:            [bool] if True save animation as gif, otherwise as images
    save:           [str] pathname of output image, or folder in case of non-collage output
    suffx:          [str] optional suffix for output file
    ----------------------------------------------------------------------------------------------------- """
    lat_1   = encoder.predict( file_to_array( img_1, in_rgb( encoder ) ) )
    lat_2   = encoder.predict( file_to_array( img_2, in_rgb( encoder ) ) )

    # interpolated latents
    rng     = np.linspace( 0, 1, num=steps )
    lats    = [ ( 1-i ) * lat_1 + i * lat_2 for i in rng ]                      

    # new images - case of simple VAE model
    if not isinstance( decoder, ( list, tuple ) ):
        imgs        = [ decoder.predict( l ) for l in lats ]                            
        imgs        = [ array_to_image( i, out_rgb( decoder ) ) for i in imgs ]

    # FIXME use new function latent_to_image instead!!!

    # new images - case of MultiVAE model
    else:
        dec_rgb, dec_car, dec_lane      = decoder
        lats_rgb, lats_car, lats_lane   = [ [], [] ,[] ]

        for l in lats:
            ll      = model_obj.split_latent( K.variable( l ), split=model_obj.split )
            lats_rgb.append( K.eval( ll[ 0 ] ) )
            lats_car.append( K.eval( ll[ 1 ] ) )
            lats_lane.append( K.eval( ll[ 2 ] ) )

        img_rgb     = [ dec_rgb.predict( l ) for l in lats_rgb ]                            
        img_car     = [ dec_car.predict( l ) for l in lats_car ]                            
        img_lane    = [ dec_lane.predict( l ) for l in lats_lane ]                            
        imgs        = [ img_overlays( r, c, l ) for r, c, l in zip( img_rgb, img_car, img_lane ) ]

    if not isinstance( decoder, ( list, tuple ) ):
        h, w    = imgsize( decoder, inp=False )
    else:
        h, w    = imgsize( decoder[ 0 ], inp=False )

    if not os.path.isdir( save ):
        os.makedirs( save )

    # save results as animated gif
    if gif:
        imgs_anim   = imgs + imgs[ ::-1 ]

        i1r         = Image.open( img_1 )
        # FIXME use new function get_targ()
        i1c         = Image.open( img_1.replace( 'RGB', 'SEGM_CAR' ).replace( '.jpg', '.png' ) )
        i1l         = Image.open( img_1.replace( 'RGB', 'SEGM_LANE' ).replace( '.jpg', '.png' ) )
        i1          = img_overlays( i1r, i1c, i1l )
        i2r         = Image.open( img_2 )
        i2c         = Image.open( img_2.replace( 'RGB', 'SEGM_CAR' ).replace( '.jpg', '.png' ) )
        i2l         = Image.open( img_2.replace( 'RGB', 'SEGM_LANE' ).replace( '.jpg', '.png' ) )
        i2          = img_overlays( i2r, i2c, i2l )

        imgs_coll   = [ make_collage( ( i1, c, i2 ), w, h, n_cols=3, pad_color="#FFFFFF" ) for c in imgs_anim ]
        save_gif( imgs_coll, os.path.join( save, "interp{}.gif".format( suffx ) ) )

    else:
        # add orginal images to the plot
        imgs.insert( 0, Image.open( img_1 ) )
        imgs.append( Image.open( img_2 ) )                                          

        save_collage( imgs, w, h, os.path.join( save, "interp_collage.png" ) )
        for n, i in enumerate( imgs ):
            i.save( os.path.join( save, "interp_{}.png".format( n ) ) )



def swap_latent( encoder, decoder, img_1, img_2, model_obj, swap_class, collage=True, save='swap.png' ):
    """ -----------------------------------------------------------------------------------------------------
    Compute two latent spaces from input images, swap slices of the latent codes and produce novel images

    ~ IMPORTANT NOTE: it ONLY supports split = 2 (RGB contains entirely both CAR and LANE)

    encoder:        [keras.engine.training.Model]
    decoder:        [keras.engine.training.Model or list]
    img_1:          [str] pathname of first input image file
    img_2:          [str] pathname of second input image file
    model_obj:      [arch.MultipleVAE] entire model object
    swap_class:     [list of str] code of latent space to swap
    collage:        [bool] if True save a single collage of all images, otherwise save each separately
    save:           [str] pathname of output image, or folder in case of non-collage output
    ----------------------------------------------------------------------------------------------------- """
    lat_1   = encoder.predict( file_to_array( img_1, in_rgb( encoder ) ) )
    lat_2   = encoder.predict( file_to_array( img_2, in_rgb( encoder ) ) )

    dec_rgb, dec_car, dec_lane          = decoder

    # FIXME use new function latent_to_image instead!!!

    # get slices of latent spaces
    lat_split_1, size                   = model_obj.split_latent( K.variable( lat_1 ), split=model_obj.split )
    lat_split_2, size                   = model_obj.split_latent( K.variable( lat_2 ), split=model_obj.split )
    lat_rgb_1, lat_car_1, lat_lane_1    = map( K.eval, lat_split_1 )
    lat_rgb_2, lat_car_2, lat_lane_2    = map( K.eval, lat_split_2 )

    # swap slices of latent spaces
    lat_car_a   = lat_car_2 if 'CAR' in swap_class else lat_car_1
    lat_car_b   = lat_car_1 if 'CAR' in swap_class else lat_car_2
    lat_lane_a  = lat_lane_2 if 'LANE' in swap_class else lat_lane_1
    lat_lane_b  = lat_lane_1 if 'LANE' in swap_class else lat_lane_2
    lat_rgb_a   = np.concatenate( ( lat_car_a, lat_rgb_1[ :, size[ 1 ] : -size[ 1 ] ], lat_lane_a ), axis=-1 )
    lat_rgb_b   = np.concatenate( ( lat_car_b, lat_rgb_2[ :, size[ 1 ] : -size[ 1 ] ], lat_lane_b ), axis=-1 )

    # produce images from novel latent spaces
    img_rgb_a   = dec_rgb.predict( lat_rgb_a )
    img_rgb_b   = dec_rgb.predict( lat_rgb_b )
    img_car_a   = dec_car.predict( lat_car_a )
    img_car_b   = dec_car.predict( lat_car_b )
    img_lane_a  = dec_lane.predict( lat_lane_a )
    img_lane_b  = dec_lane.predict( lat_lane_b )
    img_a       = img_overlays( img_rgb_a, img_car_a, img_lane_a )
    img_b       = img_overlays( img_rgb_b, img_car_b, img_lane_b )

    if collage:
        i1      = Image.open( img_1 )
        i2      = Image.open( img_2 )
        h, w    = imgsize( encoder, inp=True )
        save_collage( ( i1, i2, img_a, img_b ), w, h, save )

    else:
        if not os.path.isdir( save ):
            os.makedirs( save )
        img_a.save( os.path.join( save, "swap_a.png" ) )
        img_b.save( os.path.join( save, "swap_b.png" ) )



# ===========================================================================================================
#
#   Accuracy of model (for segmentation)
#
#   - accur_img
#   - accur_latent
#   - accur_tset
#   - accur_tset_time
#   - accur_sel_time
#
#   - accur_class_latent
#
# ===========================================================================================================
    
def accur_img( pred, targ, threshold=0.5, epsilon=1000 ):
    """ -----------------------------------------------------------------------------------------------------
    Return the accuracy of the prediction of a segmented image, usign as metrics:
        - Intersection over Union (IoU)
        - pixel accuracy = ( TP + TN ) / ( TP + TN + FP + FN )

    https://www.jeremyjordan.me/evaluating-image-segmentation-models/

    pred:           [numpy.ndarray] segmented prediction
    targ:           [numpy.ndarray] segmented target
    threshold:      [float] used to binarize the predicted segmentation
    epsilon:        [int] minimum number of pixels that must be in the union

    return:         [tuple] IoU and pixel accuracy scores [0..1]
    ----------------------------------------------------------------------------------------------------- """

    # binarize the arrays
    pred    = pred > threshold
    targ    = targ > threshold

    inter   = np.logical_and( targ, pred )
    union   = np.logical_or( targ, pred )
    s_inter = np.sum( inter )
    s_union = np.sum( union )

    # skip the case when there are too few pixel belonging to the class
    if s_union < epsilon:
        return np.NaN, np.NaN
    else:
        iou     = s_inter / s_union

    # TODO check if this is really working
    tr_ps   = np.sum( np.logical_and( targ, pred ) )
    tr_ng   = np.sum( np.logical_and( np.logical_not( targ ), np.logical_not( pred ) ) )
    fl_ps   = np.sum( np.logical_and( np.logical_not( targ ), pred ) )
    fl_ng   = np.sum( np.logical_and( np.logical_not( pred ), targ ) )

    pxl_acc = ( tr_ps + tr_ng ) / ( tr_ps + tr_ng + fl_ps + fl_ng )

    return iou, pxl_acc


def accur_mutliseq( model, inp_str, threshold=0.5, epsilon=1000 ):
    """ -----------------------------------------------------------------------------------------------------
    This function compute the accurary for a model that workd on MULTISEQ data (like RMVAE)
    The IoU is computed both for the input frames, decoded to themselves, and for the predicted frame(s)

    model:          [keras.engine.training.Model] RNN for time prediction in feature space
    inp_str:        [tuple of str] code of RGB input images
    threshold:      [float] used to binarize the predicted segmentation
    epsilon:        [int] minimum number of pixels that must be in the union

    return:         [list of tuple] scores (iou_car, iou_lane) for each current and future output
    ----------------------------------------------------------------------------------------------------- """

    assert len( inp_str ) == cnfg[ 'n_input' ] + cnfg[ 'n_output' ]
    inputs      = [ code_to_path( c ) for c in inp_str[ : cnfg[ 'n_input' ] ] ]
    inputs      = [ file_to_array( c, True ) for c in inputs ]

    pred_all    = model.predict( inputs )                   # all predictions (should be 3 x len( inp_str ))
    pred_car    = pred_all[ 1 : : 3 ]                       # predictions for cars (should be len( inp_str ))
    pred_lane   = pred_all[ 2 : : 3 ]                       # predictions for lanes (should be len( inp_str ))

    trg_str_c   = [ code_to_path( s.replace( "RGB", "SEGM_CAR" ) ) for s in inp_str ]
    trg_str_l   = [ code_to_path( s.replace( "RGB", "SEGM_LANE" ) ) for s in inp_str ]
    trg_arr_c   = [ file_to_array( s, True ) for s in trg_str_c ]
    trg_arr_l   = [ file_to_array( s, True ) for s in trg_str_l ]

    iou_car     = []
    iou_lane    = []

    for p, t in zip( pred_car, trg_arr_c ):
        s   = accur_img( p, t, threshold=threshold, epsilon=epsilon )
        iou_car.append( np.nan if s is None else s[ 0 ] )

    for p, t in zip( pred_lane, trg_arr_l ):
        s   = accur_img( p, t, threshold=threshold, epsilon=epsilon )
        iou_lane.append( np.nan if s is None else s[ 0 ] )

    scores      = [ ( c, l ) for c, l in zip( iou_car, iou_lane ) ]
    return scores



def accur_latent( model, dec, dec_obj, inp_str, trg_str, threshold=0.5, epsilon=1000 ):
    """ -----------------------------------------------------------------------------------------------------
    This function compute the accurary for a time recurrent model that takes as input latents and predict future latents
    In order to compute IoU the function decodes the predicted latents into images, to be compared with the segmented targets

    model:          [keras.engine.training.Model] RNN for time prediction in feature space
    dec:            [keras.engine.training.Model or list of them] *AE decoding from feature space to image space
    dec_obj:        [arch.MultipleVAE] required for splitting the latent spaces into RGB/CAR/LANE
    inp_str:        [tuple of str] code of RGB input images
    trg_str:        [tuple of str] code of RGB target images
    threshold:      [float] used to binarize the predicted segmentation
    epsilon:        [int] minimum number of pixels that must be in the union

    return:         [list of tuple] scores (iou_car, iou_lane) for each predicted output
    ----------------------------------------------------------------------------------------------------- """
    inp_lat     = np.array( [ code_to_latent( c ) for c in inp_str ] )
    inp_lat     = np.expand_dims( inp_lat, axis=0 )         # add batch dimension

    pred_lat    = model.predict( inp_lat )                  # pred in latent format
    pred_rgb    = []                                        # pred in array format
    pred_car    = []                                        # pred in array format
    pred_lane   = []                                        # pred in array format

    for l in pred_lat:
        r, c, l     = latent_to_image( l, dec, dec_obj=dec_obj, segm=True )
        pred_rgb.append( r )
        pred_car.append( c )
        pred_lane.append( l )

    # FIXME use new function get_targ()
    trg_str_c   = [ code_to_path( s.replace( "RGB", "SEGM_CAR" ) ) for s in trg_str ]
    trg_str_l   = [ code_to_path( s.replace( "RGB", "SEGM_LANE" ) ) for s in trg_str ]
    trg_arr_c   = [ file_to_array( s, True ) for s in trg_str_c ]
    trg_arr_l   = [ file_to_array( s, True ) for s in trg_str_l ]

    iou_car     = []
    iou_lane    = []

    for p, t in zip( pred_car, trg_arr_c ):
        s   = accur_img( p, t, threshold=threshold, epsilon=epsilon )
        iou_car.append( np.nan if s is None else s[ 0 ] )

    for p, t in zip( pred_lane, trg_arr_l ):
        s   = accur_img( p, t, threshold=threshold, epsilon=epsilon )
        iou_lane.append( np.nan if s is None else s[ 0 ] )

    scores      = [ ( c, l ) for c, l in zip( iou_car, iou_lane ) ]
    return scores



def accur_tset( model, input_dir, target_dir, input_cond=lambda x: True, target_cond=lambda x: True,
        epsilon=1000, fname="accur_tset.txt" ):
    """ -----------------------------------------------------------------------------------------------------
    Compute the accuracy of the model over a folder of images. Write a text file with the accuracy scores.

    model:          [keras.engine.training.Model] original model
    input_dir:      [str] folder of input images
    target_dir:     [str] folder of target images
    input_cond:     [function] condition for selecting input files
    target_cond:    [function] condition for selecting target files
    epsilon:        [int] minimum number of pixels that must be in the union (used in "accur_img")
    fname:          [str] path of output file

    return:         [dict] keys are image file names, values are the accuracy scores
    ----------------------------------------------------------------------------------------------------- """
    if not os.path.isdir( input_dir ):
        raise ValueError( "{} directory does not exist".format( input_dir ) )

    if not os.path.isdir( target_dir ):
        raise ValueError( "{} directory does not exist".format( target_dir ) )

    if not hasattr( model, 'loss' ):
        raise ValueError( "no metric found in the model" )

    i_cond  = lambda x: input_cond( x ) and cond_ext( x.lower() )
    t_cond  = lambda x: target_cond( x ) and cond_ext( x.lower() )

    # file names
    i_files = sorted( [ os.path.join( input_dir, f ) for f in os.listdir( input_dir ) if i_cond( f ) ] )
    t_files = sorted( [ os.path.join( target_dir, f ) for f in os.listdir( target_dir ) if t_cond( f ) ] )

    d   = {}

    for inp, targ in zip( i_files, t_files ):
        trg             = file_to_array( targ, out_rgb( model ) )
        prd             = pred_image( model, inp )
        accurs          = accur_img( prd, trg, epsilon=epsilon )
        if accurs is None: continue
        img_name        = os.path.basename( inp ).split( '.' )[ 0 ]     # file name without path nor extension
        d[ img_name ]   = accurs

    # write results into file
    if fname is not None:
        f   = open( fname, 'w' )
        f.write( '{:^20}\t{:^8}\t{:^6}\n'.format( 'IMAGE', 'IoU', 'PXL ACC' ) )
        f.write( 47 * '-' + '\n' )

        for kv in sorted( d.items(), key=lambda x: x[ 1 ][ 0 ] ):
            f.write( '{:^20}\t{:^8.4f}\t{:^6.4f}\n'.format( kv[ 0 ], *kv[ 1 ] ) )

        mean        = np.nanmean( np.array( list( d.values() ) ), axis=0 )  # mean scores
        d[ 'mean' ] = mean
        f.write( 47 * '-' + '\n' )
        f.write( '{:^20}\t{:^8.4f}\t{:^6.4f}\n'.format( 'mean', *mean ) )
            
        f.close()

    return d



def accur_tset_time( model, dec, dec_obj, test_str, threshold=0.5, epsilon=1000, fname="accur_tset.txt" ):
    """ -----------------------------------------------------------------------------------------------------
    Compute the accuracy of the model over a folder of images. Write a text file with the accuracy scores.

    NOTE:   this function consumes a lot of time on the large test dataset, an alternative is accur_sel_time()
            that works on a smaller selection of samples

    model:          [keras.engine.training.Model] recurrent model
    dec:            [keras.engine.training.Model or list of them] *AE decoding from feature space to image space
    dec_obj:        [arch.MultipleVAE] required for splitting the latent spaces into RGB/CAR/LANE
    test_str:       [tuple] (x,y) for testing, containing string names
    threshold:      [float] used to binarize the predicted segmentation (used in "accur_img")
    epsilon:        [int] minimum number of pixels that must be in the union (used in "accur_img")
    fname:          [str] path of output file

    return:         [dict] keys are image file names, values are the accuracy scores
    ----------------------------------------------------------------------------------------------------- """
    inp_strs        = test_str[ 0 ]
    trg_strs        = test_str[ 1 ]
    n_inp           = len( inp_strs )           # number of inputs for each sample
    n_out           = len( trg_strs )           # number of outputs for each sample
    n_smpl          = len( inp_strs[ 0 ] )      # number of samples in the dataset

    samples         = {}
    scores          = {}

    for n in range( n_smpl ):
        k       = inp_strs[ 0 ][ n ]            # the entry's key is the string of input n.1

        if k in samples:
            ms.print_err( "Found a duplicate in dict keys: " + k )

        # a sample has format [ [ i_1, ..., i_n ], [ t1, ..., t_m ] ]
        samples[ k ]    = []
        samples[ k ].append( [ inp_strs[ i ][ n ] for i in range( n_inp ) ] ) 
        samples[ k ].append( [ trg_strs[ i ][ n ] for i in range( n_out ) ] )

    # get scores of samples
    for k in samples:
        print( "doing sample " + k )
        sys.stdout.flush()
        try:
            a   = accur_latent( model, dec, dec_obj, samples[ k ][ 0 ], samples[ k ][ 1 ], threshold=threshold, epsilon=epsilon )
        except Exception as err:
            ms.print_wrn( err )
            continue
        scores[ k ] = a

    # write results into file
    if fname is not None:
        f   = open( fname, 'w' )

        # header
        f.write( '+' + 138 * '-' + '+' + '\n' )
        f.write( '|{:^24}|'.format( 'CODE F1' ) )

        for n in range( n_out ):
            s   = ' F{}'.format( n + n_inp + 1 )
            f.write( ( 2 * '{:^18}|' ).format( 'IoU CAR' + s, 'IoU LANE' + s ) )

        f.write( ( 2 * '{:^18}|' + '\n' ).format( 'mean IoU CAR', 'mean IoU LANE' ) )
        f.write( '+' + 138 * '-' + '+' + '\n' )

        # sample scores
        for k, v in sorted( scores.items(), key=lambda x: x[ 0 ] ):
            # k is the key, v is the list of scores, as returned by accur_latent

            f.write( '|{:^24}|'.format( k ) )

            for n in range( n_out ):
                f.write( ( 2 * '{:^18.4f}|' ).format( *v[ n ] ) )

            mean    = np.nanmean( np.array( v ), axis=0 )
            f.write( ( 2 * '{:^18.4f}|' + '\n' ).format( *mean ) )

        f.write( '+' + 138 * '-' + '+' + '\n' )

        # mean scores
        mean        = np.nanmean( np.array( list( scores.values() ) ), axis=0 )
        f.write( '|{:^24}|'.format( 'Mean' ) )

        for n in range( n_out ):
            f.write( ( 2 * '{:^18.4f}|' ).format( *mean[ n ] ) )
        f.write( ( 18 * ' ' + '|' + 18 * ' ' + '|\n' ) )
            
        f.write( '+' + 138 * '-' + '+' + '\n' )
        f.close()

    return scores



def accur_sel_time( model, dec, dec_obj, threshold=0.5, epsilon=1000, fname="accur_tset.txt" ):
    """ -----------------------------------------------------------------------------------------------------
    Accuracy for recurrent models working on latent variables.
    Compute the accuracy of the model over a selection of test samples, imported from exec_dset.sel_test().
    In addition, it computes aggregated accuracies over the classes CITY, FREEWAY, SUNNY, DARK, as accur_class_latent()
    Write a text file with the accuracy scores.
    This function is much like accur_tset_time(), however the selection of test samples has an associated
    category, and the scores are grouped for these categories, and in the classes.

    model:          [keras.engine.training.Model] recurrent model
    dec:            [keras.engine.training.Model or list of them] *AE decoding from feature space to image space
    dec_obj:        [arch.MultipleVAE] required for splitting the latent spaces into RGB/CAR/LANE
    threshold:      [float] used to binarize the predicted segmentation (used in "accur_img")
    epsilon:        [int] minimum number of pixels that must be in the union (used in "accur_img")
    fname:          [str] path of output file

    return:         [dict] keys are image file names, values are the accuracy scores
    ----------------------------------------------------------------------------------------------------- """
    def header( f1="CODE F1" ):
        """
        help function for printing the header
        """
        f.write( '+' + 138 * '-' + '+' + '\n' )
        f.write( '|{:^24}|'.format( f1 ) )

        for n in range( n_out ):
            s   = ' F{}'.format( n + n_inp + 1 )
            f.write( ( 2 * '{:^18}|' ).format( 'IoU CAR' + s, 'IoU LANE' + s ) )

        f.write( ( 2 * '{:^18}|' + '\n' ).format( 'mean IoU CAR', 'mean IoU LANE' ) )
        f.write( '+' + 138 * '-' + '+' + '\n' )

    stest, cats     = exec_dset.sel_test()      # generate all the string codes of the selected samples
    n_inp           = cnfg[ 'n_input' ]
    n_out           = cnfg[ 'n_output' ]
    step            = cnfg[ 'step' ]

    scores          = {}
    cat_scores      = {}

    # first slot for CAR, second for LANE
    score_all       = [ [ [], [] ] for n in range( n_out ) ]
    score_city      = [ [ [], [] ] for n in range( n_out ) ]
    score_freeway   = [ [ [], [] ] for n in range( n_out ) ]
    score_sunny     = [ [ [], [] ] for n in range( n_out ) ]
    score_dark      = [ [ [], [] ] for n in range( n_out ) ]

    for c in cats:
        cat_scores[ c ] = [ [ [], [] ] for n in range( n_out ) ]    # note the double list for car and lane scores

    # get scores of samples
    for k in stest:
        if DEBUG0:
            print( "doing sample " + k )
            sys.stdout.flush()

        f       = k                                     # initial frame code
        xs      = []
        ys      = []
        for i in range( n_inp ):
            xs.append( f )
            f   = exec_dset.next_timeframe( f, step )
        for i in range( n_out ):
            ys.append( f )
            f   = exec_dset.next_timeframe( f, step )

        try:
            acc         = accur_latent( model, dec, dec_obj, xs, ys, threshold=threshold, epsilon=epsilon )
        except Exception as err:
            ms.print_wrn( err )
            continue

        scores[ k ] = acc
        
        for n in range( n_out ):
            cat_scores[ stest[ k ] ][ n ][ 0 ].append( acc[ n ][ 0 ] )  # accumulate car scores
            cat_scores[ stest[ k ] ][ n ][ 1 ].append( acc[ n ][ 1 ] )  # accumulate lane scores

            score_all[ n ][ 0 ].append( acc[ n ][ 0 ] )
            score_all[ n ][ 1 ].append( acc[ n ][ 1 ] )

            if check_incl( k, code_city ):
                score_city[ n ][ 0 ].append( acc[ n ][ 0 ] )
                score_city[ n ][ 1 ].append( acc[ n ][ 1 ] )
            elif check_incl( k, code_freeway ):
                score_freeway[ n ][ 0 ].append( acc[ n ][ 0 ] )
                score_freeway[ n ][ 1 ].append( acc[ n ][ 1 ] )
            if check_incl( k, code_sunny ):
                score_sunny[ n ][ 0 ].append( acc[ n ][ 0 ] )
                score_sunny[ n ][ 1 ].append( acc[ n ][ 1 ] )
            elif check_incl( k, code_dark ):
                score_dark[ n ][ 0 ].append( acc[ n ][ 0 ] )
                score_dark[ n ][ 1 ].append( acc[ n ][ 1 ] )

    for n in range( n_out ):
        score_all[ n ][ 0 ]     = np.nanmean( score_all[ n ][ 0 ] )
        score_all[ n ][ 1 ]     = np.nanmean( score_all[ n ][ 1 ] )
        score_city[ n ][ 0 ]    = np.nanmean( score_city[ n ][ 0 ] )
        score_city[ n ][ 1 ]    = np.nanmean( score_city[ n ][ 1 ] )
        score_freeway[ n ][ 0 ] = np.nanmean( score_freeway[ n ][ 0 ] )
        score_freeway[ n ][ 1 ] = np.nanmean( score_freeway[ n ][ 1 ] )
        score_sunny[ n ][ 0 ]   = np.nanmean( score_sunny[ n ][ 0 ] )
        score_sunny[ n ][ 1 ]   = np.nanmean( score_sunny[ n ][ 1 ] )
        score_dark[ n ][ 0 ]    = np.nanmean( score_dark[ n ][ 0 ] )
        score_dark[ n ][ 1 ]    = np.nanmean( score_dark[ n ][ 1 ] )

        for c in cats:
            cat_scores[ c ][ n ][ 0 ]   = np.nanmean( np.array( cat_scores[ c ][ n ][ 0 ] ) )
            cat_scores[ c ][ n ][ 1 ]   = np.nanmean( np.array( cat_scores[ c ][ n ][ 1 ] ) )

    if fname is None:
        return scores, cat_scores


    # write results into file
    f   = open( fname, 'w' )
    header()

    # sample scores
    for k, v in sorted( scores.items(), key=lambda x: x[ 0 ] ):
        # k is the key, v is the list of scores, as returned by accur_latent

        f.write( '|{:^24}|'.format( k ) )

        for n in range( n_out ):
            f.write( ( 2 * '{:^18.4f}|' ).format( *v[ n ] ) )

        mean    = np.nanmean( np.array( v ), axis=0 )
        f.write( ( 2 * '{:^18.4f}|' + '\n' ).format( *mean ) )


    # mean scores
    header( f1="MEAN" )
    mean        = np.nanmean( np.array( list( scores.values() ) ), axis=0 )
    f.write( '|{:^24}|'.format( ' ' ) )

    for n in range( n_out ):
        f.write( ( 2 * '{:^18.4f}|' ).format( *mean[ n ] ) )
    f.write( ( 18 * ' ' + '|' + 18 * ' ' + '|\n' ) )
    f.write( '+' + 138 * '-' + '+' + '\n' )
        

    # categories scores
    header( f1="CATEGORY" )
    for k, v in sorted( cat_scores.items(), key=lambda x: x[ 0 ] ):
        # k is the category description, v is the list of scores averaged over category

        f.write( '|{:^24.24}|'.format( k ) )

        for n in range( n_out ):
            f.write( ( 2 * '{:^18.4f}|' ).format( *v[ n ] ) )

        mean    = np.nanmean( np.array( v ), axis=0 )
        f.write( ( 2 * '{:^18.4f}|' + '\n' ).format( *mean ) )

    f.write( '+' + 138 * '-' + '+' + '\n' )

    # general scores
    f.write( '|{:^24}|'.format( 'ALL FRAMES' ) )
    for n in range( n_out ):
        f.write( ( 2 * '{:^18.4f}|' ).format( *score_all[ n ] ) )
    f.write( '\n|{:^24}|'.format( 'CITY' ) )
    for n in range( n_out ):
        f.write( ( 2 * '{:^18.4f}|' ).format( *score_city[ n ] ) )
    f.write( '\n|{:^24}|'.format( 'FREEWAY' ) )
    for n in range( n_out ):
        f.write( ( 2 * '{:^18.4f}|' ).format( *score_freeway[ n ] ) )
    f.write( '\n|{:^24}|'.format( 'SUNNY' ) )
    for n in range( n_out ):
        f.write( ( 2 * '{:^18.4f}|' ).format( *score_sunny[ n ] ) )
    f.write( '\n|{:^24}|'.format( 'DARK' ) )
    for n in range( n_out ):
        f.write( ( 2 * '{:^18.4f}|' ).format( *score_dark[ n ] ) )

    f.close()

    return scores, cat_scores



def accur_class_multiseq( model, fname, threshold=0.5, epsilon=1000 ):
    """ -----------------------------------------------------------------------------------------------------
    Accuracy for models working on data of type MULTISEQ (like RMVAE)
    Compute the accuracy of the model over a selection of test samples, imported from exec_dset.sel_test(), and
    aggregated over the classes CITY, FREEWAY, SUNNY, DARK.
    Write a text file with the accuracy scores.

    model:          [keras.engine.training.Model] decoder(s) from latent to image space
    fname:          [str] path of output file
    threshold:      [float] used to binarize the predicted segmentation (used in "accur_img")
    epsilon:        [int] minimum number of pixels that must be in the union (used in "accur_img")

    return:         [dict] keys are image file names, values are the accuracy scores
    ----------------------------------------------------------------------------------------------------- """
    def header( f1="CODE F1" ):
        """
        help function for printing the header
        """
        f.write( '+' + 138 * '-' + '+' + '\n' )
        f.write( '|{:^24}|'.format( f1 ) )

        for n in range( n_frames ):
            s   = ' F{}'.format( n + 1 )
            f.write( ( 2 * '{:^18}|' ).format( 'IoU CAR' + s, 'IoU LANE' + s ) )

        f.write( ( 2 * '{:^18}|' + '\n' ).format( 'mean IoU CAR', 'mean IoU LANE' ) )
        f.write( '+' + 138 * '-' + '+' + '\n' )

    stest, cats     = exec_dset.sel_test()      # generate all the string codes of the selected samples
    n_frames        = cnfg[ 'n_input' ] + cnfg[ 'n_output' ]
    step            = cnfg[ 'step' ]

    scores          = {}
    cat_scores      = {}

    # first slot for CAR, second for LANE
    score_all       = [ [ [], [] ] for n in range( n_frames ) ]
    score_city      = [ [ [], [] ] for n in range( n_frames ) ]
    score_freeway   = [ [ [], [] ] for n in range( n_frames ) ]
    score_sunny     = [ [ [], [] ] for n in range( n_frames ) ]
    score_dark      = [ [ [], [] ] for n in range( n_frames ) ]

    for c in cats:
        cat_scores[ c ] = [ [ [], [] ] for n in range( n_frames ) ]    # note the double list for car and lane scores

    # get scores of samples
    for s in stest:
        if DEBUG0:
            print( "doing sample " + s )
            sys.stdout.flush()

        k       = s                                     # initial frame code
        inp_str = []
        for i in range( n_frames ):
            inp_str.append( k )
            k   = exec_dset.next_timeframe( k, step )

        try:
            acc         = accur_mutliseq( model, inp_str, threshold=threshold, epsilon=epsilon )
        except Exception as err:
            ms.print_wrn( err )
            continue

        scores[ s ] = acc
        
        for n in range( n_frames ):
            cat_scores[ stest[ s ] ][ n ][ 0 ].append( acc[ n ][ 0 ] )  # accumulate car scores
            cat_scores[ stest[ s ] ][ n ][ 1 ].append( acc[ n ][ 1 ] )  # accumulate lane scores

            score_all[ n ][ 0 ].append( acc[ n ][ 0 ] )
            score_all[ n ][ 1 ].append( acc[ n ][ 1 ] )

            if check_incl( s, code_city ):
                score_city[ n ][ 0 ].append( acc[ n ][ 0 ] )
                score_city[ n ][ 1 ].append( acc[ n ][ 1 ] )
            elif check_incl( s, code_freeway ):
                score_freeway[ n ][ 0 ].append( acc[ n ][ 0 ] )
                score_freeway[ n ][ 1 ].append( acc[ n ][ 1 ] )
            if check_incl( s, code_sunny ):
                score_sunny[ n ][ 0 ].append( acc[ n ][ 0 ] )
                score_sunny[ n ][ 1 ].append( acc[ n ][ 1 ] )
            elif check_incl( s, code_dark ):
                score_dark[ n ][ 0 ].append( acc[ n ][ 0 ] )
                score_dark[ n ][ 1 ].append( acc[ n ][ 1 ] )

    for n in range( n_frames ):
        score_all[ n ][ 0 ]     = np.nanmean( score_all[ n ][ 0 ] )
        score_all[ n ][ 1 ]     = np.nanmean( score_all[ n ][ 1 ] )
        score_city[ n ][ 0 ]    = np.nanmean( score_city[ n ][ 0 ] )
        score_city[ n ][ 1 ]    = np.nanmean( score_city[ n ][ 1 ] )
        score_freeway[ n ][ 0 ] = np.nanmean( score_freeway[ n ][ 0 ] )
        score_freeway[ n ][ 1 ] = np.nanmean( score_freeway[ n ][ 1 ] )
        score_sunny[ n ][ 0 ]   = np.nanmean( score_sunny[ n ][ 0 ] )
        score_sunny[ n ][ 1 ]   = np.nanmean( score_sunny[ n ][ 1 ] )
        score_dark[ n ][ 0 ]    = np.nanmean( score_dark[ n ][ 0 ] )
        score_dark[ n ][ 1 ]    = np.nanmean( score_dark[ n ][ 1 ] )

        for c in cats:
            cat_scores[ c ][ n ][ 0 ]   = np.nanmean( np.array( cat_scores[ c ][ n ][ 0 ] ) )
            cat_scores[ c ][ n ][ 1 ]   = np.nanmean( np.array( cat_scores[ c ][ n ][ 1 ] ) )

    if fname is None:
        return scores, cat_scores


    # write results into file
    f   = open( fname, 'w' )
    header()

    # sample scores
    for k, v in sorted( scores.items(), key=lambda x: x[ 0 ] ):
        # k is the key, v is the list of scores, as returned by accur_latent

        f.write( '|{:^24}|'.format( k ) )

        for n in range( n_frames ):
            f.write( ( 2 * '{:^18.4f}|' ).format( *v[ n ] ) )

        mean    = np.nanmean( np.array( v ), axis=0 )
        f.write( ( 2 * '{:^18.4f}|' + '\n' ).format( *mean ) )


    # mean scores
    header( f1="MEAN" )
    mean        = np.nanmean( np.array( list( scores.values() ) ), axis=0 )
    f.write( '|{:^24}|'.format( ' ' ) )

    for n in range( n_frames ):
        f.write( ( 2 * '{:^18.4f}|' ).format( *mean[ n ] ) )
    f.write( ( 18 * ' ' + '|' + 18 * ' ' + '|\n' ) )
    f.write( '+' + 138 * '-' + '+' + '\n' )
        

    # categories scores
    header( f1="CATEGORY" )
    for k, v in sorted( cat_scores.items(), key=lambda x: x[ 0 ] ):
        # k is the category description, v is the list of scores averaged over category

        f.write( '|{:^24.24}|'.format( k ) )

        for n in range( n_frames ):
            f.write( ( 2 * '{:^18.4f}|' ).format( *v[ n ] ) )

        mean    = np.nanmean( np.array( v ), axis=0 )
        f.write( ( 2 * '{:^18.4f}|' + '\n' ).format( *mean ) )

    f.write( '+' + 138 * '-' + '+' + '\n' )

    # general scores
    f.write( '|{:^24}|'.format( 'ALL FRAMES' ) )
    for n in range( n_frames ):
        f.write( ( 2 * '{:^18.4f}|' ).format( *score_all[ n ] ) )
    f.write( '\n|{:^24}|'.format( 'CITY' ) )
    for n in range( n_frames ):
        f.write( ( 2 * '{:^18.4f}|' ).format( *score_city[ n ] ) )
    f.write( '\n|{:^24}|'.format( 'FREEWAY' ) )
    for n in range( n_frames ):
        f.write( ( 2 * '{:^18.4f}|' ).format( *score_freeway[ n ] ) )
    f.write( '\n|{:^24}|'.format( 'SUNNY' ) )
    for n in range( n_frames ):
        f.write( ( 2 * '{:^18.4f}|' ).format( *score_sunny[ n ] ) )
    f.write( '\n|{:^24}|'.format( 'DARK' ) )
    for n in range( n_frames ):
        f.write( ( 2 * '{:^18.4f}|' ).format( *score_dark[ n ] ) )

    f.close()

    return scores, cat_scores



def accur_class_latent( dec, dec_obj, fname, threshold=0.5, epsilon=1000 ):
    """ -----------------------------------------------------------------------------------------------------
    Accuracy for static models working on latent variables.
    Compute the accuracy of the model over a selection of test samples, imported from exec_dset.sel_test(), and
    aggregated over the classes CITY, FREEWAY, SUNNY, DARK.
    The latent variables corresponding to the samples are found in the dataset loaded in the global dset_code_latent,
    or to be load from the file with filename cnfg[ 'decod' ]
    Write a text file with the accuracy scores.

    dec:            [keras.engine.training.Model or list of them] decoder(s) from latent to image space
    dec_obj:        [arch.MultipleVAE] required only in the case of multiple decoders
    fname:          [str] path to output file
    threshold:      [float] used to binarize the predicted segmentation (used in "accur_img")
    epsilon:        [int] minimum number of pixels that must be in the union (used in "accur_img")
    ----------------------------------------------------------------------------------------------------- """
    # generate the frames codes and the corresponding categories
    codes, descr    = exec_dset.sel_test()

    # first slot for CAR, second for LANE
    score_all       = [ [], [] ]                
    score_city      = [ [], [] ]
    score_freeway   = [ [], [] ]
    score_sunny     = [ [], [] ]
    score_dark      = [ [], [] ]
    score_descr     = {}
    for d in descr:
        score_descr[ d ]    = [ [], [] ]

    for k in codes:
        if DEBUG0:
            print( "Doing sample",  k )
            sys.stdout.flush()

        latent      = code_to_latent( k )
        pr, pc, pl  = latent_to_image( latent, dec, dec_obj=dec_obj, segm=True )    # pred
        tr, tc, tl  = get_targ( code_to_path( k ), segm=True )                      # targ
        iou_c, _    = accur_img( pc, tc, threshold=threshold, epsilon=epsilon )
        iou_l, _    = accur_img( pl, tl, threshold=threshold, epsilon=epsilon )

        score_all[ 0 ].append( iou_c )
        score_all[ 1 ].append( iou_l )
        score_descr[ codes[ k ] ][ 0 ].append( iou_c )
        score_descr[ codes[ k ] ][ 1 ].append( iou_l )

        if check_incl( k, code_city ):
            score_city[ 0 ].append( iou_c )
            score_city[ 1 ].append( iou_l )
        elif check_incl( k, code_freeway ):
            score_freeway[ 0 ].append( iou_c )
            score_freeway[ 1 ].append( iou_l )
        if check_incl( k, code_sunny ):
            score_sunny[ 0 ].append( iou_c )
            score_sunny[ 1 ].append( iou_l )
        elif check_incl( k, code_dark ):
            score_dark[ 0 ].append( iou_c )
            score_dark[ 1 ].append( iou_l )

    score_all[ 0 ]      = np.nanmean( score_all[ 0 ] )
    score_all[ 1 ]      = np.nanmean( score_all[ 1 ] )
    score_city[ 0 ]     = np.nanmean( score_city[ 0 ] )
    score_city[ 1 ]     = np.nanmean( score_city[ 1 ] )
    score_freeway[ 0 ]  = np.nanmean( score_freeway[ 0 ] )
    score_freeway[ 1 ]  = np.nanmean( score_freeway[ 1 ] )
    score_sunny[ 0 ]    = np.nanmean( score_sunny[ 0 ] )
    score_sunny[ 1 ]    = np.nanmean( score_sunny[ 1 ] )
    score_dark[ 0 ]     = np.nanmean( score_dark[ 0 ] )
    score_dark[ 1 ]     = np.nanmean( score_dark[ 1 ] )
    for d in descr:
        score_descr[ d ][ 0 ]   = np.nanmean( score_descr[ d ][ 0 ] )
        score_descr[ d ][ 1 ]   = np.nanmean( score_descr[ d ][ 1 ] )

    # write results into file
    f   = open( fname, 'w' )

    f.write( '+' + 59 * '-' + '+\n' )
    f.write( ( '|' + 31 * ' ' + '|' + 2 * '{:^13}|' + '\n' ).format( 'IoU CAR', 'IoU LANE' ) )
    f.write( '+' + 59 * '-' + '+\n' )
    f.write( ( '|{:>30} |' + 2 * '{:^13.4f}|' + '\n' ).format( 'ALL FRAMES', *score_all ) )
    f.write( ( '|{:>30} |' + 2 * '{:^13.4f}|' + '\n' ).format( 'CITY', *score_city ) )
    f.write( ( '|{:>30} |' + 2 * '{:^13.4f}|' + '\n' ).format( 'FREEWAY', *score_freeway ) )
    f.write( ( '|{:>30} |' + 2 * '{:^13.4f}|' + '\n' ).format( 'SUNNY', *score_sunny ) )
    f.write( ( '|{:>30} |' + 2 * '{:^13.4f}|' + '\n' ).format( 'DARK', *score_dark ) )
    f.write( '+' + 59 * '-' + '+\n' )

    for d in descr:
        f.write( ( '|{:>30} |' + 2 * '{:^13.4f}|' + '\n' ).format( d, *score_descr[ d ] ) )
    f.write( '+' + 59 * '-' + '+\n' )
    f.close()



# ===========================================================================================================
#
#   Evaluation of model
#
#   - eval_latent
#   - eval_latent_folder
#
#   - get_metric
#   - eval_img
#   - eval_raw
#   - eval_tset
#
# ===========================================================================================================
    
def eval_latent( fname, dec, dec_obj=None ):
    """ -----------------------------------------------------------------------------------------------------
    Given an input image file, use its correspondent latent representation in a dataset to produce an
    output image

    fname:          [str] path to RGB image file
    dec:            [keras.engine.training.Model or list of them] decoder(s) from latent to image space
    dec_obj:        [arch.MultipleVAE] required only in the case of multiple decoders

    return:         couple of [PIL.Image.Image]
    ----------------------------------------------------------------------------------------------------- """
    code    = fname.split( '/' )[ -1 ].split( '.' )[ 0 ]
    latent  = code_to_latent( code )
    pred    = latent_to_image( latent, dec, dec_obj=dec_obj, segm=False )
    targ    = get_targ( fname )

    return pred, targ



def eval_latent_folder( input_dir, save_dir, dec, dec_obj=None ):
    """ -----------------------------------------------------------------------------------------------------
    Execute the function eval_latent() on all files in a given folder.
    Save output images in another folder.

    input_dir       [str] path to folder of input files
    save_dir        [str] path to folder where to save results
    dec:            [keras.engine.training.Model or list of them] decoder(s) from latent to image space
    dec_obj:        [arch.MultipleVAE] required only in the case of multiple decoders
    ----------------------------------------------------------------------------------------------------- """
    if not os.path.isdir( input_dir ):
        raise ValueError( "{} directory does not exist".format( input_dir ) )

    if not os.path.isdir( save_dir ):
        os.makedirs( save_dir )

    fnames  = [ os.path.join( input_dir, f ) for f in sorted( os.listdir( input_dir ) ) if f.endswith( '.jpg' ) ]

    for f in fnames:
        pred, targ  = eval_latent( f, dec, dec_obj=dec_obj )
        code        = f.split( '/' )[ -1 ].split( '.' )[ 0 ][ :12 ]
        pred.save( os.path.join( save_dir, code + '_pred.png' ) )
        targ.save( os.path.join( save_dir, code + '_targ.png' ) )



def get_metric( code ):
    """ -----------------------------------------------------------------------------------------------------
    Return a metric for evaluation
    code:           [str] code of a possible metric

    return:         [keras.losses] loss function
    ----------------------------------------------------------------------------------------------------- """
    if code == 'MSE':
        return losses.mean_squared_error
    raise ValueError( "{} is note a coded metric".format( code ) )
    return False

    

def eval_img( model, i_input, i_target ):
    """ -----------------------------------------------------------------------------------------------------
    Return the model loss value, given an input image and its target image (ground truth).
    Use the keras function for evaluation, with the model's loss function.

    model:          [keras.engine.training.Model]
    i_input:        [str] path of input image file
    i_target        [str] path of target image file

    return:         [float] loss value
    ----------------------------------------------------------------------------------------------------- """
    if not hasattr( model, 'loss' ):
        raise ValueError( "no metric found in the model" )

    i1      = file_to_array( i_input, in_rgb( model ) )
    i2      = file_to_array( i_target, out_rgb( model ) )
    loss    = model.evaluate( i1, i2, batch_size=1, verbose=0 )

    return loss



def eval_multimg( model, i_rgb, i_car, i_lane ):
    """ -----------------------------------------------------------------------------------------------------
    Return the losses for a multi-model, given an input image and its target images (ground truth).
    Use the keras function for evaluation, with the model's loss function.

    model:          [keras.engine.training.Model]
    i_rgb:          [str] path of rgb input/target image file
    i_car:          [list] paths of car target image file
    i_lane:         [list] paths of lane target image file

    return:         [list] loss values
    ----------------------------------------------------------------------------------------------------- """
    if not hasattr( model, 'loss' ):
        raise ValueError( "no metric found in the model" )

    rgb     = file_to_array( i_rgb, True )
    car     = file_to_array( i_car, False )
    lane    = file_to_array( i_lane, False )
    loss    = model.evaluate( rgb, [ rgb, car, lane], batch_size=1, verbose=0 )

    return loss



def eval_raw( model, i_input, i_target, metric, multi_class=None ):
    """ -----------------------------------------------------------------------------------------------------
    Return the model loss value, given an input image and its target image (ground truth).
    Use a "raw" pixel-wise approach for evaluation, using a given metric for loss evaluation.

    It appears to be just a VERY VERY slow version of 'eval_img'

    model:          [keras.engine.training.Model]
    i_input:        [str] path of input image file
    i_target:       [str] path of target image file
    metric:         [function] metric for evaluating the prediction
    multi_class:    [str] one of {'RGB','CAR','LANE'} if model is a MultipleVAE, None otherwise

    return:         [numpy.array] loss values for each pixel of the image
    ----------------------------------------------------------------------------------------------------- """
    if multi_class is None:
        i1      = pred_image( model, i_input )
    elif multi_class == 'RGB':
        i1      = pred_image( model, i_input )[ 0 ]
    elif multi_class == 'LANE':
        i1      = pred_image( model, i_input )[ 1 ]
    elif multi_class == 'CAR':
        i1      = pred_image( model, i_input )[ 2 ]

    i2      = file_to_array( i_target, out_rgb( model ) )

    i1      = K.variable( i1 )
    i2      = K.variable( i2 )
    loss    = K.eval( metric( i1, i2 ) )

    # NOTE use 'loss.mean()' to return a single float value
    return loss



def eval_tset( model, input_dir, target_dir, metric='MSE', fname="eval_tset.txt" ):
    """ -----------------------------------------------------------------------------------------------------
    Evaluate the model over a folder of images. Write a text file with the loss values.

    model:          [keras.engine.training.Model] original model
    input_dir:      [str] folder of input images
    target_dir:     [str] folder of target images
    metric:         [str] code of a loss metric
    fname:          [str] path of output file

    return:         [dict] keys are image file names, values are the evaluation errors
    ----------------------------------------------------------------------------------------------------- """
    if not os.path.isdir( input_dir ):
        raise ValueError( "{} directory does not exist".format( input_dir ) )

    if not os.path.isdir( target_dir ):
        raise ValueError( "{} directory does not exist".format( target_dir ) )

    if not hasattr( model, 'loss' ):
        if metric is None:
            raise ValueError( "no metric found in the model" )
        model.compile( optimizer=optimizers.SGD(), loss=get_metric( metric ) )
    else:
        if metric is not None:
            model.loss = get_metric( metric )

    # file names
    i_files = sorted( [ os.path.join( input_dir, f )
            for f in os.listdir( input_dir ) if cond_ext( f.lower() ) ] )
    t_files = sorted( [ os.path.join( target_dir, f )
            for f in os.listdir( target_dir ) if cond_ext( f.lower() ) ] )

    d   = {}

    for inp, targ in zip( i_files, t_files ):
        img_name        = os.path.basename( inp ).split( '.' )[ 0 ]     # file name without path nor extension
        loss            = eval_img( model, inp, targ )
        d[ img_name ]   = loss

    # write results into file
    if fname is not None:
        f   = open( fname, 'w' )
        f.write( '{:^20}\t{:^8}\n'.format( 'IMAGE', 'LOSS' ) )
        f.write( 31 * '-' + '\n' )

        for kv in sorted( d.items(), key=lambda x: x[ 1 ] ):
            f.write( '{:^20}\t{:^8.4f}\n'.format( *kv ) )

        mean        = np.nanmean( np.array( list( d.values() ) ) )      # mean loss
        d[ 'mean' ] = mean
        f.write( 31 * '-' + '\n' )
        f.write( '{:^20}\t{:^8.4f}\n'.format( 'mean', mean ) )
            
        f.close()

    return d


def eval_multitset( model, rgb_dir, car_dir, lane_dir, metric=None, fname="eval_tset.txt" ):
    """ -----------------------------------------------------------------------------------------------------
    Evaluate the model over a folder of images. Write a text file with the loss values.

    model:          [keras.engine.training.Model] original model
    rgb_dir:        [str] folder of input images
    ...
    fname:          [str] path of output file

    return:         [dict] keys are image file names, values are the evaluation errors
    ----------------------------------------------------------------------------------------------------- """
    if not os.path.isdir( rgb_dir ):
        raise ValueError( "{} directory does not exist".format( rgb_dir ) )

    if not os.path.isdir( car_dir ):
        raise ValueError( "{} directory does not exist".format( car_dir ) )

    if not os.path.isdir( lane_dir ):
        raise ValueError( "{} directory does not exist".format( lane_dir ) )

    if not hasattr( model, 'loss' ):
        if metric is None:
            raise ValueError( "no metric found in the model" )
        model.compile( optimizer=optimizers.SGD(), loss=metric )
    else:
        if metric is not None:
            model.loss = metric

    # file names
    r_files = sorted( [ os.path.join( rgb_dir, f )
            for f in os.listdir( rgb_dir ) if cond_ext( f.lower() ) ] )
    c_files = sorted( [ os.path.join( car_dir, f )
            for f in os.listdir( car_dir ) if cond_ext( f.lower() ) ] )
    l_files = sorted( [ os.path.join( lane_dir, f )
            for f in os.listdir( lane_dir ) if cond_ext( f.lower() ) ] )

    d   = {}

    for rgb, car, lane in zip( r_files, c_files, l_files ):
        img_name        = os.path.basename( rgb ).split( '.' )[ 0 ]             # file basename
        loss            = eval_multimg( model, rgb, car, lane )
        d[ img_name ]   = loss

    # write results into file
    if fname is not None:
        f   = open( fname, 'w' )
        f.write( '{:^20}\t{:^8}{:^8}{:^8}{:^8}\n'.format( 'image', 'loss', 'rgb', 'car', 'lane' ) )
        f.write( 55 * '-' + '\n' )

        for kv in sorted( d.items(), key=lambda x: x[ 1 ][ 0 ] ):
            f.write( '{:^20}\t{:^8.4f}{:^8.4f}{:^8.4f}{:^8.4f}\n'.format( kv[ 0 ], *kv[ 1 ] ) )

        mean        = tuple( np.nanmean( np.array( list( d.values() ) ), axis=0 ) ) # mean losses
        d[ 'mean' ] = mean
        f.write( 55 * '-' + '\n' )
        f.write( '{:^20}\t{:^8.4f}{:^8.4f}{:^8.4f}{:^8.4f}\n'.format( 'mean', *mean ) )
            
        f.close()

    return d



# FIXME all this part is not verified

# ===========================================================================================================
#
#   - create_test_model
#
#   - layer_outputs
#   - layer_weights
#
#   - model_outputs
#   - model_weights
#   - model_dead
#
# ===========================================================================================================

def create_test_vae( nn ):
    """ -----------------------------------------------------------------------------------------------------
    Create two new versions of an VAE existing model, producing an output after every layer,
    useful for testing.

    nn:             [keras.engine.training.Model] original model

    return:         2 * ( [keras.engine.training.Model] ) new models for encoder and decoder
    ----------------------------------------------------------------------------------------------------- """
    submodels   = [ l.name for l in nn.layers ]
    if not 'encoder' in submodels:
        raise ValueError( "no encoder found in the model" )
    if not 'decoder' in submodels:
        raise ValueError( "no decoder found in the model" )

    encoder = nn.get_layer( 'encoder' )
    decoder = nn.get_layer( 'decoder' )
    enc_out = [ l.output for l in encoder.layers[ 1 : ] ]
    dec_out = [ l.output for l in decoder.layers[ 1 : ] ]
    enc = Model( inputs=nn.get_input_at( 0 ), outputs=enc_out )
    dec = Model( inputs=decoder.get_input_at( 0 ), outputs=dec_out )

    return enc, dec


def create_test_mvae( nn, branch='RGB' ):
    """ -----------------------------------------------------------------------------------------------------
    Create two new versions of an MVAE existing model, producing an output after every layer,
    useful for testing.

    nn:             [keras.engine.training.Model] original model
    branch:         [str] which branch ( 'RGB', 'CAR', 'LANE' )

    return:         2 * ( [keras.engine.training.Model] ) new models for encoder and decoder
    ----------------------------------------------------------------------------------------------------- """
    submodels   = [ l.name for l in nn.layers ]
    if not 'encoder' in submodels:
        raise ValueError( "no encoder found in the model" )
    dec_name    = 'decoder_' + branch.lower()
    if not dec_name in submodels:
        raise ValueError( "no {} found in the model".format( dec_name ) )

    encoder = nn.get_layer( 'encoder' )
    decoder = nn.get_layer( dec_name )
    enc_out = [ l.output for l in encoder.layers[ 1 : ] ]
    dec_out = [ l.output for l in decoder.layers[ 1 : ] ]
    enc = Model( inputs=nn.get_input_at( 0 ), outputs=enc_out )
    dec = Model( inputs=decoder.get_input_at( 0 ), outputs=dec_out )

    return enc, dec



def create_test_model( nn, branch='RGB' ):
    """ -----------------------------------------------------------------------------------------------------
    Create a new version of an existing model, producing an output after every layer,
    useful for testing.

    nn:             [keras.engine.training.Model] original model
    branch:         [str] which branch in case of multiple decoders

    return:         [keras.engine.training.Model] new model
    ----------------------------------------------------------------------------------------------------- """
    if nn.name == 'VAE':
        return create_test_vae( nn )
    if nn.name == 'MVAE':
        return create_test_mvae( nn, branch=branch )



def layer_outputs( model, layer, output, pth, normalize=True, i_size=None, prfx="out_" ):
    """ -----------------------------------------------------------------------------------------------------
    Plot the output of a convolutional layer

    model:          [keras.engine.training.Model] the result of create_test_model()
    layer:          [str] name of layer
    output:         [numpy.ndarray] convolution output
    pth:            [str] folder where to store results
    normalize:      [bool] if True, normalize each sub-plot individually
    i_size:         [tuple] image size, if not given is assumed from the model input
    prfx:           [str] prefix of output image files

    return:         [tuple] number of dead filters and total number of filters
    ----------------------------------------------------------------------------------------------------- """
    n_feat      = output.shape[ -1 ]
    if i_size is None:
        h, w        = imgsize( model )
    else:
        h, w        = i_size
    feat_list   = []
    cnt         = 0

    for f in range( n_feat ):
        pixels  = output[ 0, :, :, f ]              # activation values of current feature
        ptp     = pixels.ptp()                      # difference between min and max activation value

        if ptp == 0:
            if DEBUG0:
                ms.print_msg( cnfg[ 'log_msg' ], "Dead filter in layer {}, feature {}, activation {}".format(
                            layer, feat, ptp ) )
            cnt += 1

        elif normalize:                             # pixel normalization (only when ptp is not zero)
            pixels      = 255. * ( pixels - pixels.min() ) / ptp

        feat_list.append( Image.fromarray( pixels ) )

    fname   = prfx + layer + '.png'
    i       = save_collage( feat_list, w, h, os.path.join( pth, fname ) )

    return cnt, n_feat



# FIXME check if this still works
def layer_weights( model, layer, wght, pth, dpi=10, normalize=False, prfx="wght_" ):
    """ -----------------------------------------------------------------------------------------------------
    Plot the kernel weights of a convolution/deconvolution layer

    model:          [keras.engine.training.Model]
    layer:          [str] name of layer
    wght:           [numpy.ndarray] convolution weights
    pth:            [str] folder where to store results
    dpi:            [int] resolution of kernel images
    normalize:      [bool] if True, normalize each sub-plot individually
    prfx:           [str] prefix of output image files
    ----------------------------------------------------------------------------------------------------- """
    mx, mn      = -inf, inf
    wght        = wght.reshape( *wght.shape[ 0:2 ], -1 )    # from 4D to 3D shape
    n_wght      = wght.shape[ -1 ]
    h, w        = dpi * np.array( wght.shape[ :2 ] )
    wght_list   = []

    for v in range( n_wght ):
        pixels  = wght[ :, :, v ]

        if np.isnan( pixels.sum() ):
            ms.print_msg( cnfg[ 'log_msg' ], "Overflowed weights in layer {}, kernel {}".format( layer, v ) )
            continue

        ptp     = pixels.ptp()

        if ptp == 0:
            ms.print_msg( cnfg[ 'log_msg' ], "No active weights in layer {}, kernel {}".format( layer, v ) )

        elif normalize:
            pixels  = 255. * ( pixels - pixels.min() ) / ptp

        else:                                       # for normalizing the entire image collage
            mn      = min( pixels.min(), mn )
            mx      = max( pixels.max(), mx )

        wght_list.append( pixels )

    if not normalize:
        wght_list   = [ 255. * ( pixels - mn ) / ( mx - mn ) for pixels in wght_list ]

    wght_list   = [ Image.fromarray( pixels ) for pixels in wght_list ]

    fname   = prfx + layer + '.png'
    i       = save_collage( wght_list, w, h, os.path.join( pth, fname ) )



# FIXME check if this still works
def model_outputs( nn, img, pth, dtest=None, normalize=True ):
    """ -----------------------------------------------------------------------------------------------------
    Plot the output of all convolutional layers in the model.
    The argument 'img' can be the path of image file, or and integer index of a frame in a dataset

    nn:             [keras.engine.training.Model] (or list of), the result of create_test_model()
    img:            [str or int] image file
    pth:            [str] folder where to store results
    dtest:          [str] dataset directory (only when 'img' is an int)
    normalize:      [bool] if True, normalize each sub-plot individually
    ----------------------------------------------------------------------------------------------------- """
    if isinstance( img, int ):
        try:
            l   = [ f for f in os.listdir( dtest ) if f.lower().endswith( ( '.jpg', '.png' ) ) ]
            img =  os.path.join( dtest, l[ img ] )
        except Exception as e:
            raise e

    if not os.path.isfile( img ):
        ms.print_err( "While opening file " + img )


    if isinstance( nn, tuple ):
        enc, dec    = nn
    else:
        enc         = nn
        dec         = None

    cnt     = 0
    tot     = 0
    lay     = [ l.name for l in enc.layers[ 1: ] ]
    out     = pred_image( enc, img )

    for l, o in zip( lay, out ):
        if 'conv' in l:            # plot output only for conv layers
            c, t    = layer_outputs( enc, l, o, pth, normalize=normalize )
            cnt     += c
            tot     += t

    if dec is not None:
        i_size  = imgsize( enc )
        lay     = [ l.name for l in dec.layers[ 1: ] ]
        out     = dec.predict( out[ -1 ] )
        for l, o in zip( lay, out ):
            if 'dcnv' in l:            # plot output only for deconv layers
                c, t    = layer_outputs( dec, l, o, pth, normalize=normalize, i_size=i_size )
                cnt     += c
                tot     += t

    if DEBUG0:
        ms.print_msg( cnfg[ 'log_msg' ], "Total dead filters percentage: {:.1f}%".format( 100 * cnt / tot ) )



def model_weights( nn, pth, normalize=False ):
    """ -----------------------------------------------------------------------------------------------------
    Plot the kernel weights of all convolutional layers in the model

    nn:             [keras.engine.training.Model]
    pth:            [str] folder where to store results
    normalize:      [bool] if True, normalize each sub-plot individually
    ----------------------------------------------------------------------------------------------------- """

    if isinstance( nn, tuple ):
        enc, dec    = nn
    else:
        enc         = nn
        dec         = None

    # NOTE get_weights returns a list of (weights,biases)
    wght    = [ l.get_weights() for l in enc.layers[ 1 : ] ]
    lay     = [ l.name for l in enc.layers[ 1: ] ]

    for l, w in zip( lay, wght ):
        if 'conv' in l:             # plot weights only for convolutional layers
            layer_weights( enc, l, w[ 0 ], pth, normalize=normalize )

    if dec is not None:
        wght    = [ l.get_weights() for l in dec.layers[ 1 : ] ]
        lay     = [ l.name for l in dec.layers[ 1: ] ]
        for l, w in zip( lay, wght ):
            if 'dcnv' in l:             # plot weights only for convolutional layers
                layer_weights( dec, l, w[ 0 ], pth, normalize=normalize )



# FIXME check if this still works
def model_dead( model, layer, n_img, dtest ):
    """ -----------------------------------------------------------------------------------------------------
    Count how many features of a layer are inactive, testing the prediction on serevar input images

    model:          [keras.engine.training.Model] the result of create_test_model()
    layer:          [int] index of layer
    n_img:          [int] number of input images
    dtest:          [str] dataset directory
    ----------------------------------------------------------------------------------------------------- """

    # sample 'n_img' images, evenly spaced, from the test set
    dir_list    = [ f for f in os.listdir( dtest ) if f.lower().endswith( ( '.png', '.jpg', '.jpeg' ) ) ]
    indx        = list( map( int, np.linspace( 0, len( dir_list ), n_img, endpoint=False ) ) )
    img_list    = [ os.path.join( dtest, dir_list[ i ] ) for i in indx ]

    # outputs of layer 'layer' on the 'n_img' images
    out_list    = [ pred_image( model, i )[ layer ] for i in img_list ]

    n_feat      = out_list[ 0 ].shape[ -1 ]
    dead_feat   = np.zeros( n_feat, dtype=int )

    for out in out_list:
        for feat in range( n_feat ):
            activation  = out[ 0, :, :, feat ].ptp()
            #activation  = out[ 0, :, :, feat ].sum()
            if activation == 0:
                dead_feat[ feat ]   += 1                # count the inactive features (dead filters)

    # number of filters which result dead in at least 'threshold' images
    threshold   = n_img // 2
    dead_cnt    = sum( i >= threshold for i in dead_feat )

    ms.print_msg( cnfg[ 'log_msg' ],
                "{:.1f}% of filters in layer {} are inactive on {}/{} different images".format(
                100 * dead_cnt / n_feat, layer, threshold, n_img )
    )
